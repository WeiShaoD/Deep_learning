{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WeiShaoD/Deep_learning/blob/main/tutorials/W2D5_TimeSeriesAndNaturalLanguageProcessing/student/W2D5_Tutorial2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "hoAZvst9Zbxa"
      },
      "source": [
        "# Tutorial 2: Time series for Language\n",
        "\n",
        "**Week 2, Day 5: Time Series And Natural Language Processing**\n",
        "\n",
        "**By Neuromatch Academy**\n",
        "\n",
        "__Content creators:__ Lyle Ungar, Kelson Shilling-Scrivo, Alish Dipani\n",
        "\n",
        "__Content reviewers:__ Kelson Shilling-Scrivo \n",
        "\n",
        "__Content editors:__ Kelson Shilling-Scrivo\n",
        "\n",
        "__Production editors:__ Gagana B, Spiros Chavlis\n",
        "\n",
        "<br>\n",
        "\n",
        "_Based on Content from: Anushree Hede, Pooja Consul, Ann-Katrin Reuel_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "T_q5YcH2Zbxg"
      },
      "source": [
        "<p align='center'><img src='https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True'/></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "fLFjZGYdZbxg"
      },
      "source": [
        "----\n",
        "# Tutorial objectives\n",
        "\n",
        "Before we begin with exploring how RNNs excel at modelling sequences, we will explore some of the other ways we can model sequences, encode text, and make meaningful measurements using such encodings and embeddings. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "mGlw2QYMZbxh",
        "outputId": "197abcaf-6e2a-4e5d-ef19-5fcb3726c82b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7f4c79ff39d0>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"854\"\n",
              "            height=\"480\"\n",
              "            src=\"https://mfr.ca-1.osf.io/render?url=https://osf.io/n263c/?direct%26mode=render%26action=download%26mode=render\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "# @title Tutorial slides\n",
        "\n",
        "from IPython.display import IFrame\n",
        "IFrame(src=\"https://mfr.ca-1.osf.io/render?url=https://osf.io/n263c/?direct%26mode=render%26action=download%26mode=render\", width=854, height=480)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "YW3ItZoxZbxi"
      },
      "source": [
        "These are the slides for the videos in this tutorial. If you want to locally download the slides, click [here](https://osf.io/n263c/download)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "t5zERrLjZbxi"
      },
      "source": [
        "---\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "czfArYphZbxj"
      },
      "outputs": [],
      "source": [
        "# @title Install dependencies\n",
        "\n",
        "# @markdown There may be *errors* and/or *warnings* reported during the installation. However, they are to be ignored.\n",
        "!pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 torchtext==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html --quiet\n",
        "!pip install unidecode --quiet\n",
        "!pip install nltk --quiet\n",
        "!pip install d2l==0.16.5 --quiet\n",
        "!pip install python-Levenshtein --quiet\n",
        "\n",
        "!pip install git+https://github.com/NeuromatchAcademy/evaltools --quiet\n",
        "\n",
        "from evaltools.airtable import AirtableForm\n",
        "atform = AirtableForm('appn7VdPRseSoMXEG','W2D5_T2','https://portal.neuromatchacademy.org/api/redirect/to/b4d16463-cbc0-4a8c-b0b6-1cbaa428ee7c')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "execution": {},
        "id": "MWExqukxZbxk"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "\n",
        "import re\n",
        "import math\n",
        "import time\n",
        "import nltk\n",
        "import torch\n",
        "import random\n",
        "import string\n",
        "import unidecode\n",
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from torchtext.legacy import data, datasets\n",
        "\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "gZyfCz2TZbxk"
      },
      "outputs": [],
      "source": [
        "# @title Figure Settings\n",
        "import ipywidgets as widgets\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/content-creation/main/nma.mplstyle\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "voW-5MpQZbxl",
        "outputId": "a62e6720-6feb-46af-85ba-a1c6285ce728",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "# @title  Load Dataset from `nltk`\n",
        "# No critical warnings, so we suppress it\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "MKiFUu6GZbxm"
      },
      "outputs": [],
      "source": [
        "# @title Set random seed\n",
        "\n",
        "# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
        "\n",
        "# For DL its critical to set the random seed so that students can have a\n",
        "# baseline to compare their results to expected results.\n",
        "# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n",
        "\n",
        "# Call `set_seed` function in the exercises to ensure reproducibility.\n",
        "import random\n",
        "import torch\n",
        "\n",
        "def set_seed(seed=None, seed_torch=True):\n",
        "  \"\"\"\n",
        "  Function that controls randomness.\n",
        "  NumPy and random modules must be imported.\n",
        "\n",
        "  Args:\n",
        "    seed : Integer\n",
        "      A non-negative integer that defines the random state. Default is `None`.\n",
        "    seed_torch : Boolean\n",
        "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
        "      must be imported. Default is `True`.\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "  \"\"\"\n",
        "  if seed is None:\n",
        "    seed = np.random.choice(2 ** 32)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  if seed_torch:\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "  print(f'Random seed {seed} has been set.')\n",
        "\n",
        "# In case that `DataLoader` is used\n",
        "def seed_worker(worker_id):\n",
        "  \"\"\"\n",
        "  DataLoader will reseed workers following randomness in\n",
        "  multi-process data loading algorithm.\n",
        "\n",
        "  Args:\n",
        "    worker_id: integer\n",
        "      ID of subprocess to seed. 0 means that\n",
        "      the data will be loaded in the main process\n",
        "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  worker_seed = torch.initial_seed() % 2**32\n",
        "  np.random.seed(worker_seed)\n",
        "  random.seed(worker_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "P4nJuAOnZbxn"
      },
      "outputs": [],
      "source": [
        "# @title Set device (GPU or CPU). Execute `set_device()`\n",
        "\n",
        "# Inform the user if the notebook uses GPU or CPU.\n",
        "\n",
        "def set_device():\n",
        "  \"\"\"\n",
        "  Set the device. CUDA if available, CPU otherwise\n",
        "\n",
        "  Args:\n",
        "    None\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  if device != \"cuda\":\n",
        "    print(\"WARNING: For this notebook to perform best, \"\n",
        "        \"if possible, in the menu under `Runtime` -> \"\n",
        "        \"`Change runtime type.`  select `GPU` \")\n",
        "  else:\n",
        "    print(\"GPU is enabled in this notebook.\")\n",
        "\n",
        "  return device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "execution": {},
        "id": "bBA6oc_rZbxn",
        "outputId": "86300931-3e2e-422d-c20a-708d0de33525",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: For this notebook to perform best, if possible, in the menu under `Runtime` -> `Change runtime type.`  select `GPU` \n",
            "Random seed 2021 has been set.\n"
          ]
        }
      ],
      "source": [
        "DEVICE = set_device()\n",
        "SEED = 2021\n",
        "set_seed(seed=SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "qfEcwBN1Zbxo"
      },
      "source": [
        "# Section 1: Recurrent Neural Networks (RNNs)\n",
        "\n",
        "*Time estimate: ~25mins*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "DGgqFeXoZbxo"
      },
      "source": [
        "## Section 1.1: Introduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "BNqzhYNCZbxo",
        "outputId": "0335a56e-502c-4abd-e8a4-69aaed970325",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581,
          "referenced_widgets": [
            "2010176e30244c4693d64ee394ee4af2",
            "4b8f12585b734ae4a4e6059d37838bfd",
            "99d42bf5fc7246ecafba98deb9be6a1c",
            "051e063246564f84a36abc76e79ddf50",
            "4dcb9cdf99ff40918eeea943d7d13d35",
            "fd2aa03ad65c461180227591b2db1bd5"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tab(children=(Output(), Output()), _titles={'0': 'Youtube', '1': 'Bilibili'})"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2010176e30244c4693d64ee394ee4af2"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title Video 1: RNNs\n",
        "from ipywidgets import widgets\n",
        "\n",
        "out2 = widgets.Output()\n",
        "with out2:\n",
        "  from IPython.display import IFrame\n",
        "  class BiliVideo(IFrame):\n",
        "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
        "      self.id=id\n",
        "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
        "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "  video = BiliVideo(id=\"\", width=854, height=480, fs=1)\n",
        "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
        "  display(video)\n",
        "\n",
        "out1 = widgets.Output()\n",
        "with out1:\n",
        "  from IPython.display import YouTubeVideo\n",
        "  video = YouTubeVideo(id=\"LSMPdQvkXuk\", width=854, height=480, fs=1, rel=0)\n",
        "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "  display(video)\n",
        "\n",
        "out = widgets.Tab([out1, out2])\n",
        "out.set_title(0, 'Youtube')\n",
        "out.set_title(1, 'Bilibili')\n",
        "\n",
        "# Add event to airtable\n",
        "atform.add_event('Video 1: RNNs')\n",
        "\n",
        "display(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "qzSbGZf7Zbxp"
      },
      "source": [
        "Recurrent neural networks (RNNs), are a family of neural networks for processing sequential data. Just as a convolutional network is specialized for processing a grid of values $X$, such as an image, a recurrent neural network is specialized for processing a sequence of values. RNNs prove useful in many scenarios where other deep learning models are not effective.\n",
        "\n",
        "* Not all problems can be converted into one with fixed length inputs and outputs.\n",
        "\n",
        "* The deep learning models we have seen so far pick samples randomly. This might not be the best strategy for a task of understanding meaning from a piece of text. Words in a text occur in a sequence and therefore cannot be permuted randomly to get the meaning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "97XDZuy4Zbxp"
      },
      "source": [
        "A related idea is the use of convolution across a 1-D temporal sequence. The convolution operation allows a network to share parameters across time but is shallow. The output of convolution is a sequence where each output is a function of a small number of neighboring inputs. The idea of parameter sharing manifests in applying the same convolution kernel at each time step. Recurrent networks share parameters in a diﬀerent way. Each output is a function of the previous hidden layer, always produced using the same model and weights. This recurrent formulation results in the sharing of parameters through a very deep computational graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "dTGYm14LZbxr",
        "outputId": "6e90c1cf-fc1c-4e49-ccbe-2b55e328bbf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581,
          "referenced_widgets": [
            "7e5d884286044fa5a4033987431eaa5c",
            "8fd4468d834941229cf65021bfb6bcea",
            "f2176242fa2149d6a3d2c0f49687f2a9",
            "5e3b2a12779742539ac194b38f9452d4",
            "22d7a80c038f4751bd1c3cd65265ba40",
            "186e0442bbd943fa981e47c852054715"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tab(children=(Output(), Output()), _titles={'0': 'Youtube', '1': 'Bilibili'})"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e5d884286044fa5a4033987431eaa5c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title Video 2: RNN: Basic Architectures\n",
        "from ipywidgets import widgets\n",
        "\n",
        "out2 = widgets.Output()\n",
        "with out2:\n",
        "  from IPython.display import IFrame\n",
        "  class BiliVideo(IFrame):\n",
        "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
        "      self.id=id\n",
        "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
        "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "  video = BiliVideo(id=\"\", width=854, height=480, fs=1)\n",
        "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
        "  display(video)\n",
        "\n",
        "out1 = widgets.Output()\n",
        "with out1:\n",
        "  from IPython.display import YouTubeVideo\n",
        "  video = YouTubeVideo(id=\"T2jzzdSVJI0\", width=854, height=480, fs=1, rel=0)\n",
        "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "  display(video)\n",
        "\n",
        "out = widgets.Tab([out1, out2])\n",
        "out.set_title(0, 'Youtube')\n",
        "out.set_title(1, 'Bilibili')\n",
        "\n",
        "# Add event to airtable\n",
        "atform.add_event('Video 2: RNNs: Basic Architectures')\n",
        "\n",
        "display(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "V7ufGXYfZbxr"
      },
      "source": [
        "The following provides more data than the video (but can be skipped for now). For more detail, see the sources, the [deep learning book](https://www.deeplearningbook.org/contents/rnn.html), and [d2l.ai](https://d2l.ai/chapter_recurrent-neural-networks/rnn.html)\n",
        "\n",
        "When the recurrent network is trained to perform a task that requires predicting the future from the past, the network typically learns to use a hidden state at time step $t$, $H_t$ as a kind of lossy summary of the task-relevant aspects of the past sequence of inputs up to $t$. This summary is in general necessarily lossy, since it maps an arbitrary length sequence $(X_t, X_{t-1}, X_{t-2}, \\dots , X_{2}, X_{1})$ to a ﬁxed length vector $H_t$.\n",
        "\n",
        "We can represent the unfolded recurrence after $t$ steps with a function $G_t$:\n",
        "\n",
        "\\begin{equation}\n",
        "H_t=G_t(X_t, X_{t-1}, X_{t-2}, \\dots , X_{2}, X_{1}) = f(H_{t−1}, X_{t}; \\theta)\n",
        "\\end{equation}\n",
        "\n",
        "<br>\n",
        "\n",
        "<img width=700 src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl//main/tutorials/W2D5_TimeSeriesAndNaturalLanguageProcessing/static/rnn-2.gif\">\n",
        "\n",
        "<br>\n",
        "\n",
        "The function $g_t$ takes the whole past sequence $(X_t, X_{t-1}, X_{t-2}, \\dots , X_{2}, X_{1})$ as input and produces the current state, but the unfolded recurrent structure allows us to factorize $g_t$ into repeated application of a function $f$. The unfolding process thus introduces two major advantages:\n",
        "\n",
        "* Regardless of the sequence length, the learned model always has the same input size, because it is speciﬁed in terms of transition from one state to another state, rather than speciﬁed in terms of a variable-length history of states.\n",
        "\n",
        "* It is possible to use the same transition function $f$ with the same parameters at every time step.\n",
        "\n",
        "\n",
        "We will now formally write down the equations of a recurrent unit-\n",
        "\n",
        "\n",
        "Assume that we have a minibatch of inputs $X_t \\in R^{n \\times d}$ at time step $t$. In other words, for a minibatch of $n$ sequence examples, each row of $X_t$ corresponds to one example at time step $t$ from the sequence. Next, we denote by $H_t \\in R^{n \\times h}$ the hidden variable of time step $t$. Unlike the MLP, here we save the hidden variable $H_{t-1}$ from the previous time step and introduce a new weight parameter $W_{hh} \\in R^{h \\times h}$ to describe how to use the hidden variable of the previous time step in the current time step. Specifically, the calculation of the hidden variable of the current time step is determined by the input of the current time step together with the hidden variable of the previous time step:\n",
        "\n",
        "\\begin{equation}\n",
        "H_t = \\phi(X_t W_{xh} + H_{t-1}W_{hh} + b_h)\n",
        "\\end{equation}\n",
        "\n",
        "For time step $t$, the output of the output layer is similar to the computation in the MLP:\n",
        "\n",
        "\\begin{equation}\n",
        "O_t = H_t W_{hq} + b_q\n",
        "\\end{equation}\n",
        " \n",
        "Parameters of the RNN include the weights $W_{xh} \\in R^{d \\times h}, W_{hh} \\in R^{h \\times h}$ , and the bias $b_h \\in R^{1 \\times h}$ of the hidden layer, together with the weights $W_{hq} \\in R^{h \\times q}$ and the bias $b_q \\in R^{1 \\times q}$ of the output layer. It is worth mentioning that even at different time steps, RNNs always use these model parameters. Therefore, the parameterization cost of an RNN does not grow as the number of time steps increases.\n",
        "\n",
        "<br>\n",
        "\n",
        "<img width=700 src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl//main/tutorials/W2D5_TimeSeriesAndNaturalLanguageProcessing/static/rnn.svg\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "ZBNTKctfZbxs"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "-26NyZtnZbxs"
      },
      "source": [
        "We will use the IMDB dataset from <insert link>, which consists of a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. We will use torchtext to download the dataset and prepare it for training, validation and testing. Our goal is to build a model that performs binary classification between positive and negative movie reviews.\n",
        "\n",
        "We use `fix_length` argument to pad sentences of length less than `sentence_length` or truncate sentences of length greater than `sentence_length`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "wVwyF-WPZbxs"
      },
      "outputs": [],
      "source": [
        "# @title Load dataset function\n",
        "\n",
        "def download_osf():\n",
        "  # Download IMDB dataset from OSF\n",
        "  import tarfile, requests, os\n",
        "  url = \"https://osf.io/dvse9/download\"\n",
        "  fname = \"aclImdb_v1.tar.gz\"\n",
        "\n",
        "  print('Downloading Started...')\n",
        "  # Downloading the file by sending the request to the URL\n",
        "  r = requests.get(url, stream=True)\n",
        "\n",
        "  # Writing the file to the local file system\n",
        "  with open(fname, 'wb') as f:\n",
        "    f.write(r.content)\n",
        "  print('Downloading Completed.')\n",
        "\n",
        "  with tarfile.open(fname) as f:\n",
        "    # extracting all the files\n",
        "    print('Extracting all the files now...')\n",
        "    f.extractall('.data/imdb')  # specify which folder to extract to\n",
        "    print('Done!')\n",
        "    os.remove(fname)\n",
        "\n",
        "\n",
        "def load_dataset(sentence_length=50, batch_size=32, seed=2021):\n",
        "  \"\"\"\n",
        "  Dataset Loader\n",
        "\n",
        "  Args:\n",
        "    sentence_length: int\n",
        "      Length of sentence\n",
        "    seed: int\n",
        "      Set seed for reproducibility\n",
        "    batch_size: int\n",
        "      Batch size\n",
        "\n",
        "  Returns:\n",
        "    TEXT: Field instance\n",
        "      Text\n",
        "    vocab_size: int\n",
        "      Specifies size of TEXT\n",
        "    train_iter: BucketIterator\n",
        "      Training iterator\n",
        "    valid_iter: BucketIterator\n",
        "      Validation iterator\n",
        "    test_iter: BucketIterator\n",
        "      Test iterator\n",
        "  \"\"\"\n",
        "  download_osf()\n",
        "  TEXT = data.Field(sequential=True,\n",
        "                    tokenize=nltk.word_tokenize,\n",
        "                    lower=True,\n",
        "                    include_lengths=True,\n",
        "                    batch_first=True,\n",
        "                    fix_length=sentence_length)\n",
        "  LABEL = data.LabelField(dtype=torch.float)\n",
        "\n",
        "  train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
        "\n",
        "  # If no specific vector embeddings are specified,\n",
        "  # Torchtext initializes random vector embeddings\n",
        "  # which would get updated during training through backpropagation.\n",
        "  TEXT.build_vocab(train_data)\n",
        "  LABEL.build_vocab(train_data)\n",
        "\n",
        "  train_data, valid_data = train_data.split(split_ratio=0.7,\n",
        "                                            random_state=random.seed(seed))\n",
        "  train_iter, valid_iter, test_iter = data.BucketIterator.splits((train_data, valid_data, test_data),\n",
        "                                                                  batch_size=batch_size, sort_key=lambda x: len(x.text),\n",
        "                                                                  repeat=False, shuffle=True)\n",
        "  vocab_size = len(TEXT.vocab)\n",
        "\n",
        "  print(f\"Data loading is completed. Sentence length: {sentence_length}, \"\n",
        "        f\"Batch size: {batch_size}, and seed: {seed}\")\n",
        "\n",
        "  return TEXT, vocab_size, train_iter, valid_iter, test_iter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "76XEawcCZbxt"
      },
      "source": [
        "The cell below can take 15-30 secs to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "execution": {},
        "id": "2PVLbQ8HZbxt",
        "outputId": "3f998b5c-bb4e-4842-8334-95d32e2fd5b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Started...\n",
            "Downloading Completed.\n",
            "Extracting all the files now...\n",
            "Done!\n",
            "Data loading is completed. Sentence length: 50, Batch size: 32, and seed: 2021\n"
          ]
        }
      ],
      "source": [
        "TEXT, vocab_size, train_iter, valid_iter, test_iter = load_dataset(seed=SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "HcYbmdEsZbxt"
      },
      "source": [
        "Let's see what the data looks like. The words in the reviews are tokenized using the NLTK `word_tokenize` function.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "RQLroa8PZbxt",
        "outputId": "aa831913-9d8d-4267-caa4-a278e72b1002",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review:  it took us a couple of episodes to `` get into '' dark angel as a story and a series , since we were transitioning from the sopranos , a very different mentality framework . but , once we got with the gist of the series , we were very\n",
            "Label:  1 \n",
            "\n",
            "Review:  does any one know what the 2 sports cars were ? i think robert stack 's might have been a masseratti.rock hudson 's character told his father he was taking a job in iraq , is n't that timely ? i have had dorthy malone in my spank bank most\n",
            "Label:  1 \n",
            "\n",
            "Review:  i love the way that this game can make you literally jump out of your seat while you are playing it . the way that the screen jumps and flashes when you get hit , its very realistic while at the same time you have to remember that its just\n",
            "Label:  1 \n",
            "\n",
            "Review:  i ca n't think of much to say about this film . < br / > < br / > this was an awful movie ... i ca n't even tell you what made me decide to view it . it had so few redeeming qualities that i do n't\n",
            "Label:  0 \n",
            "\n",
            "Review:  what to say about this movie . well it is about a bunch of good students who have some bad drugs and turn into delinquent students that sell more of the bad drugs to people . two of those people have adverse effects as one turns into a toxic avenger\n",
            "Label:  0 \n",
            "\n",
            "[0: Negative Review, 1: Positive Review]\n"
          ]
        }
      ],
      "source": [
        "# @title Explore Dataset\n",
        "\n",
        "def text_from_dict(arr, dictionary):\n",
        "  text = []\n",
        "  for element in arr:\n",
        "    text.append(dictionary[element])\n",
        "  return text\n",
        "\n",
        "\n",
        "for idx, batch in enumerate(train_iter):\n",
        "  text = batch.text[0]\n",
        "  target = batch.label\n",
        "\n",
        "  for itr in range(25,30):\n",
        "    print('Review: ', ' '.join(text_from_dict(text[itr], TEXT.vocab.itos)))\n",
        "    print('Label: ', int(target[itr].item()), '\\n')\n",
        "\n",
        "  print('[0: Negative Review, 1: Positive Review]')\n",
        "  if idx == 0:\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "WCi_ADfLZbxu"
      },
      "outputs": [],
      "source": [
        "# @title Helper Functions\n",
        "# @markdown * `train(model, device, train_iter, valid_iter, epochs, learning_rate)`\n",
        "\n",
        "# @markdown * `test(model, device, test_iter)`\n",
        "\n",
        "# @markdown * `plot_train_val(x, train, val, train_label, val_label, title)`\n",
        "\n",
        "\n",
        "# training\n",
        "def train(model, device, train_iter, valid_iter, epochs, learning_rate):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  train_loss, validation_loss = [], []\n",
        "  train_acc, validation_acc = [], []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    # train\n",
        "    model.train()\n",
        "    running_loss = 0.\n",
        "    correct, total = 0, 0\n",
        "    steps = 0\n",
        "\n",
        "    for idx, batch in enumerate(train_iter):\n",
        "      text = batch.text[0]\n",
        "      target = batch.label\n",
        "      target = torch.autograd.Variable(target).long()\n",
        "      text, target = text.to(device), target.to(device)\n",
        "\n",
        "      # add micro for coding training loop\n",
        "      optimizer.zero_grad()\n",
        "      output = model(text)\n",
        "\n",
        "      loss = criterion(output, target)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      steps += 1\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      # get accuracy\n",
        "      _, predicted = torch.max(output, 1)\n",
        "      total += target.size(0)\n",
        "      correct += (predicted == target).sum().item()\n",
        "\n",
        "    train_loss.append(running_loss/len(train_iter))\n",
        "    train_acc.append(correct/total)\n",
        "\n",
        "    print(f'Epoch: {epoch + 1},  Training Loss: {running_loss/len(train_iter):.4f}, Training Accuracy: {100*correct/total: .2f}%')\n",
        "\n",
        "    # evaluate on validation data\n",
        "    model.eval()\n",
        "    running_loss = 0.\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for idx, batch in enumerate(valid_iter):\n",
        "        text = batch.text[0]\n",
        "        target = batch.label\n",
        "        target = torch.autograd.Variable(target).long()\n",
        "        text, target = text.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(text)\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # get accuracy\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "    validation_loss.append(running_loss/len(valid_iter))\n",
        "    validation_acc.append(correct/total)\n",
        "\n",
        "    print (f'Validation Loss: {running_loss/len(valid_iter):.4f}, Validation Accuracy: {100*correct/total: .2f}% \\n')\n",
        "\n",
        "  return train_loss, train_acc, validation_loss, validation_acc\n",
        "\n",
        "\n",
        "# testing\n",
        "def test(model, device, test_iter):\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for idx, batch in enumerate(test_iter):\n",
        "      text = batch.text[0]\n",
        "      target = batch.label\n",
        "      target = torch.autograd.Variable(target).long()\n",
        "      text, target = text.to(device), target.to(device)\n",
        "\n",
        "      outputs = model(text)\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      total += target.size(0)\n",
        "      correct += (predicted == target).sum().item()\n",
        "\n",
        "    acc = 100 * correct / total\n",
        "    return acc\n",
        "\n",
        "\n",
        "# helpers\n",
        "def plot_train_val(x, train, val, train_label, val_label, title):\n",
        "  plt.plot(x, train, label=train_label)\n",
        "  plt.plot(x, val, label=val_label)\n",
        "  plt.legend(loc='lower right')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.title(title)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "  parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  return parameters\n",
        "\n",
        "\n",
        "def init_weights(m):\n",
        "  if type(m) in (nn.Linear, nn.Conv1d):\n",
        "      nn.init.xavier_uniform_(m.weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "h8K6-rY3Zbxu"
      },
      "source": [
        "## Section 1.2: 1D CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "m212B9kiZbxu",
        "outputId": "cddb8a49-db2e-4420-8534-593c5509398c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581,
          "referenced_widgets": [
            "e9980375a92e41e2a5c01469f52b33c6",
            "384a15d32c904375a3c2880001aca63c",
            "1cf6392c1c1d48a090660779cb9ad5de",
            "b95df6511f51488d8cc98fe3b98ea733",
            "524ea3f3c93b4febab52a56af1a7eeb3",
            "75a416f8353d470b895f8b9fbbe05a00"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tab(children=(Output(), Output()), _titles={'0': 'Youtube', '1': 'Bilibili'})"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9980375a92e41e2a5c01469f52b33c6"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title Video 3: Time Series\n",
        "from ipywidgets import widgets\n",
        "\n",
        "out2 = widgets.Output()\n",
        "with out2:\n",
        "  from IPython.display import IFrame\n",
        "  class BiliVideo(IFrame):\n",
        "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
        "      self.id=id\n",
        "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
        "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "  video = BiliVideo(id=\"\", width=854, height=480, fs=1)\n",
        "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
        "  display(video)\n",
        "\n",
        "out1 = widgets.Output()\n",
        "with out1:\n",
        "  from IPython.display import YouTubeVideo\n",
        "  video = YouTubeVideo(id=\"JAjuG0o4_Xc\", width=854, height=480, fs=1, rel=0)\n",
        "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "  display(video)\n",
        "\n",
        "out = widgets.Tab([out1, out2])\n",
        "out.set_title(0, 'Youtube')\n",
        "out.set_title(1, 'Bilibili')\n",
        "\n",
        "# Add event to airtable\n",
        "atform.add_event('Video 3: Time Series')\n",
        "\n",
        "display(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "Plo2SOcCZbxv"
      },
      "source": [
        "Over the last tutorials, you were introduced to CNNs and used them to work on a range of interesting deep learning applications in vision. You also discussed where else these networks might be useful. Can we apply CNNs to language?\n",
        "\n",
        "Before we jump into RNNs we will create a CNN model for the text classification task. Let us understand how a one-dimensional convolutional layer works.\n",
        "\n",
        "<img width=700 src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl//main/tutorials/W2D5_TimeSeriesAndNaturalLanguageProcessing/static/conv1d.svg\">\n",
        "\n",
        "The above figure shows one-dimensional cross-correlation operation. The shaded parts are the first output element as well as the input and kernel array elements used in its calculation: $0 \\cdot 1 + 1 \\cdot 2 = 2$.\n",
        "\n",
        "<img width=700 src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl//main/tutorials/W2D5_TimeSeriesAndNaturalLanguageProcessing/static/conv1d-channel.svg\">\n",
        "\n",
        "The above figure shows one-dimensional cross-correlation operation with three input channels. The shaded parts are the first output element as well as the input and kernel array elements used in its calculation: $0 \\cdot 1 + 1 \\cdot 2 \\cdot 1 \\cdot 3 + 2 \\cdot 4 + 2 \\cdot (−1) + 3 \\cdot (−3) = 2$.\n",
        "\n",
        "Similarly, we have a one-dimensional pooling layer. The max-over-time pooling layer used in CNN actually corresponds to a one-dimensional global maximum pooling layer. Assuming that the input contains multiple channels, and each channel consists of values on different time steps, the output of each channel will be the largest value of all time steps in the channel. Therefore, the input of the max-over-time pooling layer can have different time steps on each channel.\n",
        "\n",
        "Fitting all the pieces together a 1D CNN architecture looks like the image below.\n",
        "\n",
        "<img width=700 src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl//main/tutorials/W2D5_TimeSeriesAndNaturalLanguageProcessing/static/textcnn.svg\">\n",
        "\n",
        "In this figure, we have an input sequence of 11 words, each with 6 channels. Our model consists of two Conv1D layers applied to this input: one of kernel size 2 having 4 output channels and one of kernel size 4 having 5 output channels.  The max-over-time pooling is applied to each of the output channels. The max-pooled outputs for each output channel of the Conv1D layers are concatenated to form 1x4 and 1x5 vectors respectively. Finally these outputs are further concatenated to form a vector of 1x9 and this is passed to a fully connected layer of size 2 for binary classification. \n",
        "\n",
        "The intuition of using a max-pooling over different channels here is to capture the most important features over time in an input sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "l7BOu_CVZbxv"
      },
      "source": [
        "### Coding Exercise 1: Implement a 1D CNN\n",
        "\n",
        "Now it's your turn to implement a 1D CNN. \n",
        "\n",
        "* Here we will use [`nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)  layer instead of pretrained word embeddings. This is a design choice. Using `nn.Embedding` layer in the network allows us to train word embeddings specific to the problem at hand. You are given the `vocab_size` which is the size of the dictionary of embeddings, and the `embed_size` which is the size of each embedding vector. \n",
        "\n",
        "* The 1D CNN should work for any number kernel size and corresponsing number of channels. Both `kernel_sizes` and `num_channels` are lists. Each element of these lists corresponds to the kernel size and output channels of a Conv1D layer.\n",
        "\n",
        "* Use [adaptive max pooling](https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool1d.html) \n",
        "\n",
        "* Determine the size of inputs and outputs to the fully-connected layer using the reference example given above.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "execution": {},
        "id": "DhzsDoZyZbxv",
        "outputId": "239c2cd8-bcde-4d99-e8a1-3cee1ee9cd16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TextCNN(\n",
            "  (embedding): Embedding(1000, 300)\n",
            "  (fc): Linear(in_features=60, out_features=2, bias=True)\n",
            "  (pool): AdaptiveMaxPool1d(output_size=1)\n",
            "  (relu): ReLU()\n",
            "  (convs): ModuleList(\n",
            "    (0): Conv1d(300, 10, kernel_size=(1,), stride=(1,))\n",
            "    (1): Conv1d(300, 20, kernel_size=(2,), stride=(1,))\n",
            "    (2): Conv1d(300, 30, kernel_size=(3,), stride=(1,))\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class TextCNN(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_size, kernel_sizes, num_channels,\n",
        "                **kwargs):\n",
        "    super(TextCNN, self).__init__(**kwargs)\n",
        "    ####################################################################\n",
        "    # Fill in missing code below (...),\n",
        "    # then remove or comment the line below to test your function\n",
        "    #raise NotImplementedError(\"TextCNN\")\n",
        "    ####################################################################\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "    self.fc = nn.Linear(sum(num_channels), 2)\n",
        "    self.pool = nn.AdaptiveMaxPool1d(1)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.convs = nn.ModuleList()\n",
        "    # This for loop adds the Conv1D layers to your network\n",
        "    for c, k in zip(num_channels, kernel_sizes):\n",
        "      # Conv1d(in_channels, out_channels, kernel_size)\n",
        "      self.convs.append(nn.Conv1d(embed_size, c, k))\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    embeddings = self.embedding(inputs)\n",
        "    embeddings = embeddings.permute(0, 2, 1)\n",
        "    # Concatenating the average-pooled outputs\n",
        "    encoding = torch.cat([\n",
        "        torch.squeeze(self.relu(self.pool(conv(embeddings))), dim=-1)\n",
        "        for conv in self.convs], dim=1)\n",
        "    outputs = self.fc(encoding)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "## Uncomment to test\n",
        "sampleCNN = TextCNN(1000, 300, [1, 2, 3], [10, 20, 30])\n",
        "print(sampleCNN)\n",
        "\n",
        "# add event to airtable\n",
        "atform.add_event('Coding Exercise 1: Implement a 1D CNN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "0isTvnuYZbxw"
      },
      "source": [
        "Sample output\n",
        "\n",
        "```python\n",
        "TextCNN(\n",
        "  (embedding): Embedding(1000, 300)\n",
        "  (fc): Linear(in_features=60, out_features=2, bias=True)\n",
        "  (pool): AdaptiveMaxPool1d(output_size=1)\n",
        "  (relu): ReLU()\n",
        "  (convs): ModuleList(\n",
        "    (0): Conv1d(300, 10, kernel_size=(1,), stride=(1,))\n",
        "    (1): Conv1d(300, 20, kernel_size=(2,), stride=(1,))\n",
        "    (2): Conv1d(300, 30, kernel_size=(3,), stride=(1,))\n",
        "  )\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "jBDqOmXjZbxw"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W2D5_TimeSeriesAndNaturalLanguageProcessing/solutions/W2D5_Tutorial2_Solution_673f6dee.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "yOtjGLidZbxw",
        "outputId": "c91ed401-5dd4-48f7-8b5e-a972f0a2ad9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1,  Training Loss: 0.7899, Training Accuracy:  50.05%\n",
            "Validation Loss: 0.7668, Validation Accuracy:  51.17% \n",
            "\n",
            "Epoch: 2,  Training Loss: 0.7554, Training Accuracy:  51.95%\n",
            "Validation Loss: 0.7528, Validation Accuracy:  52.41% \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Model hyperparameters\n",
        "learning_rate = 0.00001\n",
        "embedding_length = 100\n",
        "kernel_sizes = [3, 4, 5]\n",
        "nums_channels = [100, 100, 100]\n",
        "epochs = 10\n",
        "\n",
        "# Initialize model, training and testing\n",
        "cnn_model = TextCNN(vocab_size, embedding_length, kernel_sizes, nums_channels)\n",
        "cnn_model.to(DEVICE)\n",
        "cnn_model.apply(init_weights)\n",
        "cnn_start_time = time.time()\n",
        "cnn_train_loss, cnn_train_acc, cnn_validation_loss, cnn_validation_acc = train(cnn_model, DEVICE, train_iter, valid_iter, epochs, learning_rate)\n",
        "print(f\"--- Time taken to train = {time.time() - cnn_start_time} seconds ---\")\n",
        "test_accuracy = test(cnn_model, DEVICE, test_iter)\n",
        "print(f'Test Accuracy: {test_accuracy}%')\n",
        "\n",
        "# Plot accuracies\n",
        "plot_train_val(np.arange(0, epochs),\n",
        "               cnn_train_acc,\n",
        "               cnn_validation_acc,\n",
        "               'training_accuracy',\n",
        "               'validation_accuracy',\n",
        "               'CNN on IMDB text classification')\n",
        "\n",
        "# Number of parameters in model\n",
        "paramters = count_parameters(cnn_model)\n",
        "print(f'\\n\\nNumber of parameters = {paramters}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "6buOZYwlZbxw"
      },
      "source": [
        "## Section 1.3: Vanilla RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "y5CdnFC-Zbxw"
      },
      "source": [
        "### Coding Exercise 2: Implement a Vanilla RNN\n",
        "\n",
        "Now it's your turn to write a Vanilla RNN using PyTorch.\n",
        "\n",
        "* Once again we will use `nn.Embedding`. You are given the `vocab_size` which is the size of the dictionary of embeddings, and the `embed_size` which is the size of each embedding vector.  \n",
        "\n",
        "* Add 2 [RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) layers. This would mean stacking two RNNs together to form a stacked RNN, with the second RNN taking in outputs of the first RNN and computing the final results.\n",
        "\n",
        "* Determine the size of inputs and outputs to the fully-connected layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "zPRi3GXiZbxx"
      },
      "outputs": [],
      "source": [
        "class VanillaRNN(nn.Module):\n",
        "  def __init__(self, output_size, hidden_size, vocab_size, embed_size):\n",
        "    super(VanillaRNN, self).__init__()\n",
        "      ####################################################################\n",
        "    # Fill in missing code below (...),\n",
        "    # then remove or comment the line below to test your function\n",
        "    #raise NotImplementedError(\"Vanilla RNN\")\n",
        "    ####################################################################\n",
        "    self.hidden_size = hidden_size\n",
        "    self.word_embeddings = nn.Embedding(vocab_size, embed_size)\n",
        "    self.rnn = nn.RNN(embed_size, hidden_size, num_layers=2)\n",
        "    self.fc = nn.Linear(2*hidden_size, output_size)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    input = self.word_embeddings(inputs)\n",
        "    input = input.permute(1, 0, 2)\n",
        "    h_0 =  Variable(torch.zeros(2, input.size()[1], self.hidden_size).to(DEVICE))\n",
        "    output, h_n = self.rnn(input, h_0)\n",
        "    h_n = h_n.permute(1, 0, 2)\n",
        "    h_n = h_n.contiguous().view(h_n.size()[0], h_n.size()[1]*h_n.size()[2])\n",
        "    logits = self.fc(h_n)\n",
        "\n",
        "    return logits\n",
        "\n",
        "\n",
        "## Uncomment to test\n",
        "sampleRNN = VanillaRNN(10, 50, 1000, 300)\n",
        "print(sampleRNN)\n",
        "\n",
        "# add event to airtable\n",
        "atform.add_event('Coding Exercise 2: Implement a Vanilla RNN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "xNgDRsTbZbxx"
      },
      "source": [
        "Sample output\n",
        "\n",
        "```python\n",
        "VanillaRNN(\n",
        "  (word_embeddings): Embedding(1000, 300)\n",
        "  (rnn): RNN(300, 50, num_layers=2)\n",
        "  (fc): Linear(in_features=100, out_features=10, bias=True)\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "EeEm_hD7Zbxx"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W2D5_TimeSeriesAndNaturalLanguageProcessing/solutions/W2D5_Tutorial2_Solution_374cc479.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "vbmIGiNlZbxx"
      },
      "outputs": [],
      "source": [
        "# Model hyperparamters\n",
        "learning_rate = 0.0002\n",
        "output_size = 2\n",
        "hidden_size = 100\n",
        "embedding_length = 100\n",
        "epochs = 10\n",
        "\n",
        "# Initialize model, training and testing\n",
        "vanilla_rnn_model = VanillaRNN(output_size, hidden_size, vocab_size, embedding_length)\n",
        "vanilla_rnn_model.to(DEVICE)\n",
        "vanilla_rnn_start_time = time.time()\n",
        "vanilla_train_loss, vanilla_train_acc, vanilla_validation_loss, vanilla_validation_acc = train(vanilla_rnn_model, DEVICE, train_iter, valid_iter, epochs, learning_rate)\n",
        "print(f\"--- Time taken to train = {time.time() - vanilla_rnn_start_time} seconds ---\")\n",
        "test_accuracy = test(vanilla_rnn_model, DEVICE, test_iter)\n",
        "print(f'Test Accuracy: {test_accuracy}%')\n",
        "\n",
        "# Plot accuracy curves\n",
        "plot_train_val(np.arange(0, epochs), vanilla_train_acc, vanilla_validation_acc,\n",
        "               'training_accuracy', 'validation_accuracy',\n",
        "               'Vanilla RNN on IMDB text classification')\n",
        "\n",
        "# Number of model parameters\n",
        "paramters = count_parameters(vanilla_rnn_model)\n",
        "print(f'\\n\\nNumber of parameters = {paramters}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "03PIHAA_Zbxx"
      },
      "source": [
        "# Section 2: RNN applications and Language Models\n",
        "\n",
        "*Time estimate: ~40 mins*\n",
        "\n",
        "### **Important!** \n",
        "\n",
        "In this section, you will use your knowledge of recurrent neural networks and build some interesting NLP applications! For the remainder of the tutorial we will be switching from word-level models to character-level models; which means the text will be tokenized at the character level. We do this in the interest of simplifying our task by limiting our vocabulary (which is now a set of characters instead of words).   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "Rd0zWjs4Zbxx"
      },
      "outputs": [],
      "source": [
        "# @title Video 4: RNN Architectures for NLP\n",
        "from ipywidgets import widgets\n",
        "\n",
        "out2 = widgets.Output()\n",
        "with out2:\n",
        "  from IPython.display import IFrame\n",
        "  class BiliVideo(IFrame):\n",
        "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
        "      self.id=id\n",
        "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
        "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "  video = BiliVideo(id=\"\", width=854, height=480, fs=1)\n",
        "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
        "  display(video)\n",
        "\n",
        "out1 = widgets.Output()\n",
        "with out1:\n",
        "  from IPython.display import YouTubeVideo\n",
        "  video = YouTubeVideo(id=\"R3Dg7wBQQoU\", width=854, height=480, fs=1, rel=0)\n",
        "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "  display(video)\n",
        "\n",
        "out = widgets.Tab([out1, out2])\n",
        "out.set_title(0, 'Youtube')\n",
        "out.set_title(1, 'Bilibili')\n",
        "\n",
        "# Add event to airtable\n",
        "atform.add_event('Video 4: RNN Architectures for NLP')\n",
        "\n",
        "display(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "t3339KTiZbxx"
      },
      "source": [
        "## Section 2.1: Text Generation and Language Modelling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "F99oiBftZbxy"
      },
      "outputs": [],
      "source": [
        "# @title Video 5: Sequence Modeling\n",
        "from ipywidgets import widgets\n",
        "\n",
        "out2 = widgets.Output()\n",
        "with out2:\n",
        "  from IPython.display import IFrame\n",
        "  class BiliVideo(IFrame):\n",
        "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
        "      self.id=id\n",
        "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
        "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "  video = BiliVideo(id=\"\", width=854, height=480, fs=1)\n",
        "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
        "  display(video)\n",
        "\n",
        "out1 = widgets.Output()\n",
        "with out1:\n",
        "  from IPython.display import YouTubeVideo\n",
        "  video = YouTubeVideo(id=\"wYuvMGQdpI4\", width=854, height=480, fs=1, rel=0)\n",
        "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "  display(video)\n",
        "\n",
        "out = widgets.Tab([out1, out2])\n",
        "out.set_title(0, 'Youtube')\n",
        "out.set_title(1, 'Bilibili')\n",
        "\n",
        "# Add event to airtable\n",
        "atform.add_event('Video 5: Sequence Modeling')\n",
        "\n",
        "display(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "bJg7O3eUZbxy"
      },
      "source": [
        "The first application we will discuss in this section is that of text generation using neural language models. In linguistic theory, a language model is a probability distribution over sequences of words. The task for the model is posed as follows: given a history or context of words, can you predict the next word in the sequence? \n",
        "\n",
        "Recurrent neural networks are a natural choice for this task, since they have the ability to capture information from past observations through the hidden state. Andrej Karpathy's [blog post](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) is an excellent read for developing intuition for this task. For this exercise, you will train a character-level text generation model, give it an initial \"start\" string, and watch how it generates characters! \n",
        "\n",
        "At any given time step, the model takes one character and a hidden state as input. We convert the output of the model to a probability distribution over the possible characters, and pick a character from this distribution as the next prediction. This \"generated\" character is then passed to the model as the input in the next time step. This process is repeated for a fixed number of time steps.  (In real generation, these is often a special \"stop character\" that determines when to stop generation.   \n",
        "\n",
        "<br>\n",
        "\n",
        "<img width=700 src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl//main/tutorials/W2D5_TimeSeriesAndNaturalLanguageProcessing/static/rnnW2D5T2.png\"> \n",
        "\n",
        "\n",
        "**Note:** Language models are generally evaluated using a metric known as [perplexity](https://towardsdatascience.com/perplexity-intuition-and-derivation-105dd481c8f3), which is 2 to the entropy of the probability distribution of the language model, measured in bits.\n",
        "\n",
        "References:\n",
        "- [RNN PyTorch tutorial](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html)\n",
        "- [RNN d2l.ai tutorial](https://d2l.ai/chapter_recurrent-neural-networks/rnn.html#rnn-based-character-level-language-models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "r67mf0YGZbxy"
      },
      "outputs": [],
      "source": [
        "# @markdown Preparing the input (run me)\n",
        "\n",
        "import requests\n",
        "\n",
        "url = \"https://osf.io/f6z3p/download\"\n",
        "fname = \"1522-0.txt\"\n",
        "r = requests.get(url, stream=True)\n",
        "with open(fname, \"wb\") as f:\n",
        "  f.write(r.content)\n",
        "print(f\"The file '{fname}' has been downloaded.\")\n",
        "\n",
        "\n",
        "# Print a random chunk from the training data\n",
        "def random_chunk(chunk_len):\n",
        "  start_index = random.randint(0, file_len - chunk_len)\n",
        "  end_index = start_index + chunk_len + 1\n",
        "  return file[start_index:end_index]\n",
        "\n",
        "\n",
        "# Turn string into list of longs\n",
        "def char_tensor(string):\n",
        "  tensor = torch.zeros(len(string)).long()\n",
        "  for c in range(len(string)):\n",
        "    tensor[c] = all_characters.index(string[c])\n",
        "  return tensor\n",
        "\n",
        "\n",
        "# Get a random chunk from the traning data,\n",
        "# Convert its first n-1 chars into input char tensor\n",
        "# Convert its last n-1 chars into target char tensor\n",
        "def random_training_set(chunk_len):\n",
        "  chunk = random_chunk(chunk_len)\n",
        "  inp = char_tensor(chunk[:-1])\n",
        "  target = char_tensor(chunk[1:])\n",
        "  return inp, target\n",
        "\n",
        "\n",
        "# Read the input training file - Julius Caesar\n",
        "file = unidecode.unidecode(open('1522-0.txt').read()).lower()\n",
        "file = re.sub(r'[^a-z]+', ' ', file)\n",
        "file_len = len(file)\n",
        "print(f'\\nLength of {fname} file is {file_len}')\n",
        "\n",
        "chunk_len = 100\n",
        "print(f\"\\nRandom chunk: {random_chunk(chunk_len)}\")\n",
        "\n",
        "# Get all printable characters for generation\n",
        "# all_characters = string.printable\n",
        "all_characters = string.ascii_lowercase\n",
        "all_characters += ' '\n",
        "n_characters = len(all_characters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "fgtovcuoZbxy"
      },
      "source": [
        "**Network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "NPbsXy_-Zbxy"
      },
      "outputs": [],
      "source": [
        "class GenerationRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "    super(GenerationRNN, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "    self.encoder = nn.Embedding(input_size, hidden_size)\n",
        "    self.rnn = nn.RNN(hidden_size, hidden_size, n_layers)\n",
        "    self.decoder = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, input, hidden):\n",
        "    input = self.encoder(input.view(1, -1))\n",
        "    output, hidden = self.rnn(input, hidden)\n",
        "    output = self.decoder(hidden.view(1, -1))\n",
        "    return output, hidden\n",
        "\n",
        "  def init_hidden(self):\n",
        "    return torch.zeros(self.n_layers, 1, self.hidden_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "CunRG-oBZbxz"
      },
      "source": [
        "### Coding Exercise 3: Implement Text generation\n",
        "\n",
        "Examine the network example above and write the function below that:\n",
        "* Takes in a `prime_str` and builds up a hidden state \n",
        "* Uses the built up hidden state to iteratively generate `predict_len` number of characters from the model\n",
        "* To predict the next state, softmax the output from the model and pick the character with the maximum probability "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "zG4ffk9SZbxz"
      },
      "outputs": [],
      "source": [
        "def evaluate(net, all_characters, prime_str, predict_len):\n",
        "  hidden = net.init_hidden()\n",
        "  predicted = prime_str\n",
        "\n",
        "  # \"Building up\" the hidden state\n",
        "  for p in range(len(prime_str) - 1):\n",
        "    inp = char_tensor(prime_str[p])\n",
        "    _, hidden = net(inp, hidden)\n",
        "\n",
        "  # Tensorize of the last character\n",
        "  inp = char_tensor(prime_str[-1])\n",
        "\n",
        "  # For every index to predict\n",
        "  for p in range(predict_len):\n",
        "    ####################################################################\n",
        "    # Fill in missing code below (...),\n",
        "    # then remove or comment the line below to test your function\n",
        "    #raise NotImplementedError(\"Generation\")\n",
        "    ####################################################################\n",
        "    # Pass the inputs to the model\n",
        "    output, hidden = net(inp, hidden)\n",
        "\n",
        "    # Pick the character with the highest probability\n",
        "    top_i = torch.argmax(torch.softmax(output, dim=1))\n",
        "\n",
        "    # Add predicted character to string and use as next input\n",
        "    predicted_char = all_characters[top_i]\n",
        "    predicted += predicted_char\n",
        "    inp = char_tensor(predicted_char)\n",
        "\n",
        "  return predicted\n",
        "\n",
        "\n",
        "## Uncomment to run\n",
        "sampleDecoder = GenerationRNN(27, 100, 27, 1)\n",
        "text = evaluate(sampleDecoder, all_characters, 'hi', 10)\n",
        "if text.startswith('hi') and len(text) == 12:\n",
        "  print('Success!')\n",
        "else:\n",
        "  print('Need to change.')\n",
        "\n",
        "# add event to airtable\n",
        "atform.add_event('Coding Exercise 3: Implement Text generation')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "8iB0DxcMZbxz"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W2D5_TimeSeriesAndNaturalLanguageProcessing/solutions/W2D5_Tutorial2_Solution_afcb2da5.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "eMLa95hEZbxz"
      },
      "outputs": [],
      "source": [
        "# @markdown Run this cell to train the model\n",
        "\n",
        "def grad_clipping(net, theta):\n",
        "  \"\"\"Clip the gradient.\"\"\"\n",
        "  params = [p for p in net.parameters() if p.requires_grad]\n",
        "\n",
        "  norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
        "\n",
        "  if norm > theta:\n",
        "    for param in params:\n",
        "      param.grad[:] *= theta / norm\n",
        "\n",
        "\n",
        "# Single training step\n",
        "def train_gen(inp, target, chunk_len):\n",
        "  # Initialize hidden state, zero the gradients of decoder\n",
        "  hidden = decoder.init_hidden()\n",
        "  decoder.zero_grad()\n",
        "  loss = 0\n",
        "  # For each character in our chunk (except last), compute the hidden and ouput\n",
        "  # Using each output, compute the loss with the corresponding target\n",
        "  for c in range(chunk_len):\n",
        "    output, hidden = decoder(inp[c], hidden)\n",
        "    loss += criterion(output, target[c].unsqueeze(0))\n",
        "\n",
        "  # Backpropagate, clip gradient and optimize\n",
        "  loss.backward()\n",
        "  grad_clipping(decoder, 1)\n",
        "  decoder_optimizer.step()\n",
        "\n",
        "  # Return average loss\n",
        "  return loss.data.item() / chunk_len\n",
        "\n",
        "\n",
        "n_epochs = 3000\n",
        "hidden_size = 100\n",
        "n_layers = 1\n",
        "lr = 0.0005\n",
        "print_every, plot_every = 500, 10\n",
        "\n",
        "# Create model, optimizer and loss function\n",
        "decoder = GenerationRNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "all_losses = []\n",
        "loss_avg = 0\n",
        "\n",
        "# For every epoch\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  # Get a random (input, target) pair from training set and perform one training iteration\n",
        "  loss = train_gen(*random_training_set(chunk_len), chunk_len)\n",
        "  loss_avg += loss\n",
        "\n",
        "  if epoch % print_every == 0:\n",
        "    text = evaluate(decoder, all_characters, 'th', 50)\n",
        "    print(f'Epoch {epoch} --------------------\\n\\t {text}')\n",
        "\n",
        "  if epoch % plot_every == 0:\n",
        "    all_losses.append(loss_avg / plot_every)\n",
        "    loss_avg = 0\n",
        "\n",
        "print('\\n')\n",
        "plt.figure()\n",
        "plt.plot(all_losses)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training loss for text generation')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "UvB6kZP0Zbxz"
      },
      "source": [
        "## Section 2.2: Sequence Tagging\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "iijQc6waZbx0"
      },
      "outputs": [],
      "source": [
        "# @title Video 6: Tagging\n",
        "from ipywidgets import widgets\n",
        "\n",
        "out2 = widgets.Output()\n",
        "with out2:\n",
        "  from IPython.display import IFrame\n",
        "  class BiliVideo(IFrame):\n",
        "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
        "      self.id=id\n",
        "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
        "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "  video = BiliVideo(id=\"\", width=854, height=480, fs=1)\n",
        "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
        "  display(video)\n",
        "\n",
        "out1 = widgets.Output()\n",
        "with out1:\n",
        "  from IPython.display import YouTubeVideo\n",
        "  video = YouTubeVideo(id=\"Wnj9XZdtV7I\", width=854, height=480, fs=1, rel=0)\n",
        "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "  display(video)\n",
        "\n",
        "out = widgets.Tab([out1, out2])\n",
        "out.set_title(0, 'Youtube')\n",
        "out.set_title(1, 'Bilibili')\n",
        "\n",
        "# Add event to airtable\n",
        "atform.add_event('Video 6: Tagging')\n",
        "\n",
        "display(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "ezSssL1HZbx0"
      },
      "source": [
        "Sequence tagging is a task in machine learning whose goal is to assign a label (or category) to each unit of a sequence processed in a model. In natural language processing, some popular tagging tasks include parts-of-speech tagging and named entity recognition. \n",
        "\n",
        "Formally, we study tagging as a different task from language modeling or text generation, but there are similarities between them. Instead of choosing the next best character from the output model, we pass the hidden state of each recurrent unit from the model to a fully connected layer and assign it a label.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "zGBRHfQ5Zbx0"
      },
      "outputs": [],
      "source": [
        "# @markdown Build the dataloader (run me!)\n",
        "class TaggingDataset(torch.utils.data.Dataset):\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "\n",
        "  def __init__(self, raw_data):\n",
        "    'Initialization'\n",
        "    self.data = raw_data\n",
        "    self.vowels = ['a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U']\n",
        "\n",
        "  def __len__(self):\n",
        "    'Denotes the total number of samples'\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    'Generates one sample of data'\n",
        "    # Select sample\n",
        "    inp = char_tensor(self.data[index][:-1])\n",
        "    target = []\n",
        "    for c in self.data[index][1:]:\n",
        "      if c in self.vowels:\n",
        "        target.append(0)\n",
        "      elif c == ' ':\n",
        "        target.append(1)\n",
        "      else:\n",
        "        target.append(2)\n",
        "    target = torch.tensor(target)\n",
        "    # print(self.data[index][:-1], target)\n",
        "    return inp, target\n",
        "\n",
        "\n",
        "# Data loaders\n",
        "raw_data = []\n",
        "for i in range(100):\n",
        "  raw_data.append(random_chunk(chunk_len))\n",
        "dataset = TaggingDataset(raw_data)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "GWJYN3zwZbx0"
      },
      "source": [
        "### Coding Exercise 4: Implement Sequence Labelling\n",
        "\n",
        "For this exercise, you will train a character-level sequence labeling model on the same dataset as the previous exercise.\n",
        "\n",
        "The model takes in a character and a hidden state vector as input and predicts whether the **next** character in the sequence will be a vowel (V), space (S), or other (O). Much of the code for this task is identical to the code for text generation, so you will be making the most critical tweaks in the text generation pipeline that will give you a tagging pipeline. Note that the task becomes simpler since we are reducing the prediction space to just three classes.\n",
        "\n",
        "Structurally, the tagging model looks similar to the text generation one, except the Linear layer must map to the number of tags or labels defined for your task. Functionally, the hidden state (instead of the output) must be mapped to the Linear layer to perform the classification of the required unit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "execution": {},
        "id": "uqmEgqc3Zbx0"
      },
      "outputs": [],
      "source": [
        "class TaggingRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "    super(TaggingRNN, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.n_layers = n_layers\n",
        "    ####################################################################\n",
        "    # Fill in missing code below (...),\n",
        "    # then remove or comment the line below to test your function\n",
        "    # raise NotImplementedError(\"Tagging Init\")\n",
        "    ####################################################################\n",
        "    self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "    # https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
        "    self.rnn = nn.RNN(hidden_size, hidden_size, n_layers)\n",
        "    self.linear = nn.Linear(hidden_size, 3)\n",
        "\n",
        "  def forward(self, input, hidden):\n",
        "    input = self.embedding(input.view(1, -1))\n",
        "    output, hidden = self.rnn(input.view(1, 1, -1), hidden)\n",
        "    out = self.linear(hidden)\n",
        "    return out, hidden\n",
        "\n",
        "  def init_hidden(self):\n",
        "    return torch.zeros(self.n_layers, 1, self.hidden_size)\n",
        "## Uncomment to run\n",
        "sampleTagger = TaggingRNN(20, 100, 10, 2)\n",
        "print(sampleTagger)\n",
        "\n",
        "# add event to airtable\n",
        "atform.add_event('Coding Exercise 4: Implement Sequence Labelling')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "3wImbCrIZbx0"
      },
      "source": [
        "Sample output\n",
        "\n",
        "```python\n",
        "TaggingRNN(\n",
        "  (embedding): Embedding(20, 100)\n",
        "  (rnn): RNN(100, 100, num_layers=2)\n",
        "  (linear): Linear(in_features=100, out_features=3, bias=True)\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "Jg6ST2hjZbx1"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W2D5_TimeSeriesAndNaturalLanguageProcessing/solutions/W2D5_Tutorial2_Solution_de81767a.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "sTEnzT3yZbx1"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "Here is a function that performs the \"Vowel/Space/Other\" tagging task for each character of the given string. You don't need to code anything.  \n",
        "\n",
        "**Bonus task:** notice the similarities between it and the evaluation code for the text generation task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "x5fLh9BtZbx1"
      },
      "outputs": [],
      "source": [
        "# @markdown Evaluation function\n",
        "\n",
        "def evaluate(model, prime_str, predict_len):\n",
        "  labels = ['Vowel', 'Space', 'Other']\n",
        "\n",
        "  hidden = model.init_hidden()\n",
        "  prime_input = char_tensor(prime_str)\n",
        "  build_up = len(prime_str) - predict_len\n",
        "  predicted = prime_str[:build_up]\n",
        "\n",
        "  # Use priming string to \"build up\" hidden state\n",
        "  for p in range(build_up):\n",
        "    _, hidden = model(prime_input[p], hidden)\n",
        "\n",
        "  column_width = len('milan_is_a_nice_place')+2\n",
        "\n",
        "  print('Text {}'.ljust(column_width) + 'Next Character is ...\\n-------------------------')\n",
        "\n",
        "  # For each character remaining to be tagged\n",
        "  for p in range(0, predict_len):\n",
        "    # Get it's input tensor\n",
        "    inp = prime_input[build_up + p]\n",
        "    next_char = prime_str[build_up + p]\n",
        "    predicted += next_char\n",
        "\n",
        "    # Pass the input and the previous hidden state to the model\n",
        "    out, hidden = model(inp, hidden)\n",
        "\n",
        "    # Softmax the output and find the best tag\n",
        "    softmax = torch.softmax(out[0], dim=1, dtype=torch.float)\n",
        "    i = torch.argmax(softmax)\n",
        "    label = labels[i]\n",
        "\n",
        "    text = predicted.replace(' ', '_')\n",
        "    print(text.ljust(column_width) + label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "jEZcsaaYZbx1"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "vb_JYV5uZbx1"
      },
      "outputs": [],
      "source": [
        "# @markdown Run this cell to train the model!\n",
        "\n",
        "# Single training step\n",
        "def train_simple(inp, target, chunk_len):\n",
        "  # Initialize hidden state, zero the gradients of decoder\n",
        "  hidden = tagger.init_hidden()\n",
        "  tagger.zero_grad()\n",
        "  loss = 0\n",
        "\n",
        "  # For each character in our chunk (except last), compute the hidden and ouput\n",
        "  # Using each output, compute the loss with the corresponding target\n",
        "  for c in range(chunk_len):\n",
        "    out, hidden = tagger(inp[c], hidden)\n",
        "    loss += criterion(out[0], target[c].unsqueeze(0))\n",
        "\n",
        "  # Backpropagate and optimize\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # Return average loss\n",
        "  return loss.data.item() / chunk_len\n",
        "\n",
        "\n",
        "# Define the parameters\n",
        "n_epochs = 12\n",
        "hidden_size = 100\n",
        "n_layers = 1\n",
        "lr = 0.005\n",
        "\n",
        "print_every, plot_every = 5, 1\n",
        "\n",
        "# Create model, optimizer and loss function\n",
        "tagger = TaggingRNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "optimizer = torch.optim.Adam(tagger.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "all_losses = []\n",
        "loss_avg = 0\n",
        "\n",
        "# For every epoch\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "\n",
        "  for i, (input, target) in enumerate(dataloader):\n",
        "    loss = train_simple(input[0], target[0], chunk_len)\n",
        "    loss_avg += loss\n",
        "\n",
        "  if epoch % print_every == 0:\n",
        "    print(f'\\n------ Epoch {epoch}')\n",
        "    evaluate(tagger, 'milan is a nice place', 15)\n",
        "\n",
        "  if epoch % plot_every == 0:\n",
        "    all_losses.append(loss_avg / plot_every)\n",
        "    loss_avg = 0\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Cross Entropy Loss')\n",
        "plt.title('Training loss for sequence labelling')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "i4Zu9g8dZbx1"
      },
      "source": [
        "## Section 2.3: Sequence to Sequence \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "OEm4flJ3Zbx1"
      },
      "outputs": [],
      "source": [
        "# @title Video 7: Seq2Seq\n",
        "from ipywidgets import widgets\n",
        "\n",
        "out2 = widgets.Output()\n",
        "with out2:\n",
        "  from IPython.display import IFrame\n",
        "  class BiliVideo(IFrame):\n",
        "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
        "      self.id=id\n",
        "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
        "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "  video = BiliVideo(id=\"\", width=854, height=480, fs=1)\n",
        "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
        "  display(video)\n",
        "\n",
        "out1 = widgets.Output()\n",
        "with out1:\n",
        "  from IPython.display import YouTubeVideo\n",
        "  video = YouTubeVideo(id=\"6uJHra_esJU\", width=854, height=480, fs=1, rel=0)\n",
        "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "  display(video)\n",
        "\n",
        "out = widgets.Tab([out1, out2])\n",
        "out.set_title(0, 'Youtube')\n",
        "out.set_title(1, 'Bilibili')\n",
        "\n",
        "# Add event to airtable\n",
        "atform.add_event('Video 7: seq2seq')\n",
        "\n",
        "display(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "BpagLv-9Zbx2"
      },
      "source": [
        "Sequence-to-sequence models take in a sequence of items (words, characters, etc) as input and produces another sequence of items as output. The most \n",
        "simple seq2seq models are composed of two parts: the encoder, the context (\"state\" in the figure) and the decoder. The encoder and decoder usually consist of recurrent units that we've seen before (RNNs, GRUs or LSTMs). A high-level schematic of the architecture is as follows:\n",
        "\n",
        "<br>\n",
        "\n",
        "<img width=700 src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl//main/tutorials/W2D5_TimeSeriesAndNaturalLanguageProcessing/static/encdecW2D5T2.png\"> \n",
        "\n",
        "The encoder's recurrent unit processes the input one item at a time. Once the entire sequence is processed, the final hidden state vector produced is known as a context vector. The size of the context vector is defined while setting up the model, and is equal to the number of hidden states used in the encoder RNN. The encoder then passes the context to the decoder. The decoder's recurrent unit uses the context to produce the items for the output sequence one by one.\n",
        "\n",
        "Sources:\n",
        "- [d2l.ai on encoders](https://d2l.ai/chapter_recurrent-modern/encoder-decoder.html)\n",
        "- [d2l.ai on seq2seq](https://d2l.ai/chapter_recurrent-modern/seq2seq.html)\n",
        "- [Jalammar's blog](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "nVCMcKvJZbx2"
      },
      "source": [
        "One of the most popular applications of seq2seq models is \"machine translation\": the task of taking in a sentence in one language (the source) and producing its translation in another language (the target), with words in both languages being the sequence units. This is a supervised learning task and requires the dataset to have \"parallel sentences\", i.e., each sentence in the source language must be labelled with its translation in the target language. \n",
        "\n",
        "[Here](https://i.imgur.com/HJ6t8up.mp4) is an intuitive visualization for understanding **seq2seq** models for machine translation from English to French.\n",
        "\n",
        "Since the vocabulary of an entire language is huge, training such models to give meaningful performance requires significant time and resources. In this section, you will train a seq2seq model to perform machine translation from English to [Pig-Latin](https://en.wikipedia.org/wiki/Pig_Latin). We will modify the task to perform character-level machine translation so that vocabulary size does not grow exponentially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "D4_zd3DJZbx2"
      },
      "outputs": [],
      "source": [
        "# @title Data download and Preprocessing\n",
        "\n",
        "import os, zipfile, requests, io\n",
        "\n",
        "url = \"https://osf.io/fkg3y/download\"\n",
        "fname = \"wordlist-en_US-large-2020.12.07.zip\"\n",
        "\n",
        "print('Downloading Started...')\n",
        "# Downloading the file by sending the request to the URL\n",
        "r = requests.get(url, stream=True)\n",
        "\n",
        "# Writing the file to the local file system\n",
        "with open(fname, 'wb') as f:\n",
        "  f.write(r.content)\n",
        "print('Downloading Completed.')\n",
        "\n",
        "# opening the zip file in READ mode\n",
        "with zipfile.ZipFile(fname, 'r') as zipObj:\n",
        "  # extracting all the files\n",
        "  print('Extracting all the files now...')\n",
        "  zipObj.extractall()\n",
        "  print('Done!')\n",
        "  os.remove(fname)\n",
        "\n",
        "\n",
        "def t(str):\n",
        "  return str[0] + str[1]\n",
        "\n",
        "\n",
        "def pig_latinize(word):\n",
        "  lst = ['sh', 'gl', 'ch', 'ph', 'tr', 'br', 'fr', 'bl', 'gr', 'st', 'sl', 'cl', 'pl', 'fl']\n",
        "\n",
        "  i = word\n",
        "  if i[0] in ['a', 'e', 'i', 'o', 'u']:\n",
        "    word = i+'ay'\n",
        "  elif t(i) in lst:\n",
        "    word = i[2:] + i[:2] + 'ay'\n",
        "  elif i.isalpha() == False:\n",
        "    word = i\n",
        "  else:\n",
        "    word = i[1:] + i[0] + 'ay'\n",
        "  return word\n",
        "\n",
        "def read_data(fname='en_US-large.txt'):\n",
        "  word_list = []\n",
        "  with open(fname) as f:\n",
        "    word_list.extend(f.readlines())\n",
        "  clean_wordlist = [unidecode.unidecode(w.strip().lower()) for w in word_list if w.strip().isalpha() and len(w.strip()) > 2 and len(w.strip()) < 6]\n",
        "  clean_data = []\n",
        "  for word in clean_wordlist:\n",
        "    clean_data.append(word + ' ' + pig_latinize(word))\n",
        "  return clean_data\n",
        "\n",
        "\n",
        "def tokenize_nmt(text, num_examples=None):\n",
        "  \"\"\"Tokenize the English-French dataset.\"\"\"\n",
        "  source, target = [], []\n",
        "  source_char_set = set()\n",
        "  target_char_set = set()\n",
        "  for i, line in enumerate(text):\n",
        "    if num_examples and i > num_examples:\n",
        "      break\n",
        "    parts = line.split(' ')\n",
        "    # parts = line.split('\\t')\n",
        "    if len(parts) == 2:\n",
        "      src_txt, tgt_txt = parts\n",
        "      cur_src, cur_tgt = [], []\n",
        "      for c in src_txt:\n",
        "        cur_src.append(c)\n",
        "        source_char_set.add(c)\n",
        "      for c in tgt_txt:\n",
        "        cur_tgt.append(c)\n",
        "        target_char_set.add(c)\n",
        "      source.append(cur_src)\n",
        "      target.append(cur_tgt)\n",
        "\n",
        "  special_tokens = ['<eos>', '<bos>', '<pad>']\n",
        "  for tok in special_tokens:\n",
        "    source_char_set.add(tok)\n",
        "    target_char_set.add(tok)\n",
        "  return source, target, sorted(list(source_char_set)), sorted(list(target_char_set))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "NXrwlqiDZbx2"
      },
      "source": [
        "The following cell retrieves about 29,000 random English words and the corresponding Pig Latin translations. We then tokenize each word and its translation; in this case, our tokens are characters. We also create vocabularies for the source and target languages; and a two-way mapping for each (index to token and token to index). Finally, we pick the value for `NUM_STEPS` as the size of the largest sequence in either language. This value would mark the maximum size of the sequence accepted by our models.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "PiRq3rmVZbx2"
      },
      "outputs": [],
      "source": [
        "clean_data = read_data()\n",
        "\n",
        "source, target, source_vocab, target_vocab = tokenize_nmt(clean_data)\n",
        "\n",
        "source_idx2token = dict((i, char) for i, char in enumerate(source_vocab))\n",
        "target_idx2token = dict((i, char) for i, char in enumerate(target_vocab))\n",
        "source_vocab = dict((char, i) for i, char in enumerate(source_vocab))\n",
        "target_vocab = dict((char, i) for i, char in enumerate(target_vocab))\n",
        "\n",
        "NUM_STEPS = max(max([len(s) for s in source]), max([len(t) for t in target]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "yAH67Vv7Zbx2"
      },
      "source": [
        "### Padding\n",
        "\n",
        "The input sequences to our model can vary in length, so it is often convenient to have a consistent length among all inputs to the model (This is not required by recurrent models, but makes it easier to control minibatch size). If our defined maximum sequence length is $M$ and a given input sequence is less than that, then we pad it with zeros (until its length becomes $M$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "nKVb2fvoZbx2"
      },
      "outputs": [],
      "source": [
        "# @markdown Padding function\n",
        "def truncate_pad(line, num_steps, padding_token):\n",
        "  \"\"\"Truncate or pad sequences.\"\"\"\n",
        "  # Truncate\n",
        "  if len(line) > num_steps:\n",
        "    return line[:num_steps]\n",
        "\n",
        "  # Pad\n",
        "  number_of_pad_tokens = num_steps - len(line)\n",
        "  padding_list = [padding_token] * number_of_pad_tokens\n",
        "\n",
        "  return line + padding_list\n",
        "\n",
        "\n",
        "# Test the function\n",
        "word = ['e', 't', 'u', 'i', 's']\n",
        "input = [source_vocab[c] for c in word]\n",
        "print('Input test: ', word)\n",
        "o1 = truncate_pad(input, 10, source_vocab['<pad>'])\n",
        "x = [source_idx2token[i] for i in o1]\n",
        "print(x)\n",
        "o1 = truncate_pad(input, 1, source_vocab['<pad>'])\n",
        "x = [source_idx2token[i] for i in o1]\n",
        "print(x)\n",
        "o1 = truncate_pad(input, 5, source_vocab['<pad>'])\n",
        "x = [source_idx2token[i] for i in o1]\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "UGWB5GNzZbx3"
      },
      "outputs": [],
      "source": [
        "# @markdown Build the dataloaders\n",
        "\n",
        "def build_array(data, vocab, num_steps):\n",
        "  \"\"\"Transform text sequences of machine translation into minibatches.\"\"\"\n",
        "  complete_data, lengths = [], []\n",
        "  for lines in data:\n",
        "    lines = [vocab[l] for l in lines]\n",
        "    lines.append(vocab['<eos>'])\n",
        "    array = torch.tensor(truncate_pad(lines, num_steps, vocab['<pad>']))\n",
        "    valid_len = (array != vocab['<pad>']).type(torch.int32).sum(0)\n",
        "    complete_data.append(array)\n",
        "    lengths.append(valid_len.item())\n",
        "  return torch.stack(complete_data), torch.tensor(lengths)\n",
        "\n",
        "\n",
        "class MTDataset(torch.utils.data.Dataset):\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "\n",
        "  def __init__(self, source, target):\n",
        "    'Initialization'\n",
        "    source_data, source_lens = build_array(source, source_vocab, NUM_STEPS)\n",
        "    target_data, target_lens = build_array(target, target_vocab, NUM_STEPS)\n",
        "    self.source = source_data\n",
        "    self.source_len = source_lens\n",
        "    self.target = target_data\n",
        "    self.target_len = target_lens\n",
        "\n",
        "  def __len__(self):\n",
        "    'Denotes the total number of samples'\n",
        "    return len(self.source)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    'Generates one sample of data'\n",
        "    # Select sample\n",
        "    return self.source[index], self.source_len[index], self.target[index], self.target_len[index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "6MVhnXLWZbx3"
      },
      "source": [
        "### Encoder and Decoder Architecture\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "l3U7odHkZbx3"
      },
      "source": [
        "#### Coding Exercise 5: Implement Encoder and Decoder\n",
        "\n",
        "Implement the forward functions for the Encoder and Decoder of the seq2seq model as directed. \n",
        "\n",
        "The Encoder model is very similar to what you have seen so far:\n",
        "* Get the embedding of the input `X`\n",
        "* Pass `X` through the recurrent unit. You can define your own initial hidden state or omit it (in which case a tensor of zeros is used). \n",
        "* There is no linear layer \n",
        "\n",
        "You will notice that the Decoder also contains an embedding layer; something that we did not mention during the initial explanation of the seq2seq model, as we want to apply \"teacher forcing\" to this problem. Teacher forcing is a strategy for training RNNs that uses the model output from a previous time step as an input. Specifically, \n",
        "* Get the embedding of `X` (which is output from the previous time step)\n",
        "* Concatenate it with the previous hidden state\n",
        "* To the recurrent unit of the Decoder, pass this concatenation as \"input\"; and pass the previous hidden state as \"hidden\"\n",
        "* Pass the output of the recurrent unit through the linear layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "WUiSFSmJZbx3"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  \"\"\"The RNN encoder for sequence to sequence learning.\"\"\"\n",
        "  def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
        "                dropout=0):\n",
        "    super(Encoder, self).__init__()\n",
        "    # Embedding layer\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "    self.rnn = nn.RNN(embed_size, num_hiddens, num_layers,\n",
        "                      dropout=dropout)\n",
        "\n",
        "  def forward(self, X):\n",
        "    # The output `X` shape: (`batch_size`, `num_steps`, `embed_size`)\n",
        "    X = self.embedding(X)\n",
        "    # In RNN models, the first axis corresponds to time steps\n",
        "    X = X.permute(1, 0, 2)\n",
        "    # When state is not mentioned, it defaults to zeros\n",
        "    output, state = self.rnn(X)\n",
        "    # `output` shape: (`num_steps`, `batch_size`, `num_hiddens`)\n",
        "    # `state` shape: (`num_layers`, `batch_size`, `num_hiddens`)\n",
        "    return output, state\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  \"\"\"The RNN decoder for sequence to sequence learning.\"\"\"\n",
        "  def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
        "                dropout=0):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "    self.rnn = nn.RNN(embed_size + num_hiddens, num_hiddens, num_layers,\n",
        "                      dropout=dropout)\n",
        "    self.dense = nn.Linear(num_hiddens, vocab_size)\n",
        "    self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "  def init_state(self, enc_outputs):\n",
        "    return enc_outputs[1]\n",
        "\n",
        "  def forward(self, X, state):\n",
        "    \"\"\"Hint: always make sure your sizes are correct\"\"\"\n",
        "    # The output `X` shape: (`num_steps`, `batch_size`, `embed_size`)\n",
        "    X = self.embedding(X).permute(1, 0, 2)\n",
        "    # Broadcast `context` so it has the same `num_steps` as `X`\n",
        "    context = state[-1].repeat(X.shape[0], 1, 1)\n",
        "\n",
        "    # Concatenate X and context\n",
        "    X_and_context = torch.cat((X, context), 2)\n",
        "\n",
        "    # Recurrent unit\n",
        "    output, state = self.rnn(X_and_context, state)\n",
        "    # Linear layer\n",
        "    output = self.dense(output).permute(1, 0, 2)\n",
        "    # `output` shape: (`batch_size`, `num_steps`, `vocab_size`)\n",
        "    # `state` shape: (`num_layers`, `batch_size`, `num_hiddens`)\n",
        "\n",
        "    return output, state\n",
        "\n",
        "\n",
        "## Uncomment to run\n",
        "encoder = Encoder(1000, 300, 100, 2, 0.1)\n",
        "decoder = Decoder(1000, 300, 100, 2, 0.1)\n",
        "print(encoder)\n",
        "print(decoder)\n",
        "\n",
        "# add event to airtable\n",
        "atform.add_event('Coding Exercise 5: Implement Encoder and Decoder')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "UzTuB1t7Zbx3"
      },
      "source": [
        "Sample output\n",
        "\n",
        "```python\n",
        "Encoder(\n",
        "  (embedding): Embedding(1000, 300)\n",
        "  (rnn): RNN(300, 100, num_layers=2, dropout=0.1)\n",
        ")\n",
        "Decoder(\n",
        "  (embedding): Embedding(1000, 300)\n",
        "  (rnn): RNN(400, 100, num_layers=2, dropout=0.1)\n",
        "  (dense): Linear(in_features=100, out_features=1000, bias=True)\n",
        "  (dropout): Dropout(p=0.25, inplace=False)\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "3N_w8NG9Zbx3"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W2D5_TimeSeriesAndNaturalLanguageProcessing/solutions/W2D5_Tutorial2_Solution_750f0205.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "WSXS79P-Zbx4"
      },
      "outputs": [],
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "  \"\"\"The base class for the encoder-decoder architecture.\"\"\"\n",
        "  def __init__(self, encoder, decoder):\n",
        "    super(EncoderDecoder, self).__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def forward(self, enc_X, dec_X):\n",
        "    enc_outputs = self.encoder(enc_X)\n",
        "    dec_state = self.decoder.init_state(enc_outputs)\n",
        "    return self.decoder(dec_X, dec_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "7jcb19FlZbx4"
      },
      "outputs": [],
      "source": [
        "# @markdown Masked Loss Function\n",
        "def sequence_mask(X, valid_len, value=0):\n",
        "  \"\"\"Mask irrelevant entries in sequences.\"\"\"\n",
        "  maxlen = X.size(1)\n",
        "  mask = torch.arange((maxlen), dtype=torch.float32,\n",
        "                      device=X.device)[None, :] < valid_len[:, None]\n",
        "  X[~mask] = value\n",
        "  return X\n",
        "\n",
        "\n",
        "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
        "  \"\"\"The softmax cross-entropy loss with masks.\"\"\"\n",
        "  # `pred` shape: (`batch_size`, `num_steps`, `vocab_size`)\n",
        "  # `label` shape: (`batch_size`, `num_steps`)\n",
        "  # `valid_len` shape: (`batch_size`,)\n",
        "  def forward(self, pred, label, valid_len):\n",
        "    weights = torch.ones_like(label)\n",
        "    weights = sequence_mask(weights, valid_len)\n",
        "    self.reduction='none'\n",
        "    unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(\n",
        "        pred.permute(0, 2, 1), label\n",
        "        )\n",
        "    weighted_loss = (unweighted_loss * weights).mean(dim=1)\n",
        "    return weighted_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "QB2_MKUZZbx4"
      },
      "outputs": [],
      "source": [
        "# @markdown `train_seq2seq(model, data_loader, lr, num_epochs, tgt_vocab, device)`\n",
        "\n",
        "def train_seq2seq(model, data_loader, lr, num_epochs, tgt_vocab, device):\n",
        "  \"\"\"Train a model for sequence to sequence.\"\"\"\n",
        "  def xavier_init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "      nn.init.xavier_uniform_(m.weight)\n",
        "    if type(m) == nn.GRU:\n",
        "      for param in m._flat_weights_names:\n",
        "        if \"weight\" in param:\n",
        "          nn.init.xavier_uniform_(m._parameters[param])\n",
        "  model.apply(xavier_init_weights)\n",
        "  model.to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "  loss = MaskedSoftmaxCELoss()\n",
        "\n",
        "  dataset = MTDataset(source, target)\n",
        "  dataloader = torch.utils.data.DataLoader(dataset, batch_size=64)\n",
        "\n",
        "  model.train()\n",
        "  animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
        "                          xlim=[10, num_epochs])\n",
        "  for epoch in range(num_epochs):\n",
        "    # TODO: without d2l?\n",
        "    timer = d2l.Timer()\n",
        "    metric = d2l.Accumulator(2)  # Sum of training loss, no. of tokens\n",
        "    for batch in dataloader:\n",
        "      X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\n",
        "      bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0],\n",
        "                          device=device).reshape(-1, 1)\n",
        "\n",
        "      dec_input = torch.cat([bos, Y[:, :-1]], 1)  # Teacher forcing\n",
        "      Y_hat, _ = model(X, dec_input)\n",
        "      l = loss(Y_hat, Y, Y_valid_len)\n",
        "      l.sum().backward()  # Make the loss scalar for `backward`\n",
        "\n",
        "      grad_clipping(model, 1)\n",
        "      num_tokens = Y_valid_len.sum()\n",
        "      optimizer.step()\n",
        "      with torch.no_grad():\n",
        "        metric.add(l.sum(), num_tokens)\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "      animator.add(epoch + 1, (metric[0] / metric[1],))\n",
        "\n",
        "  print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '\n",
        "        f'tokens/sec on {str(device)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "XH4sfJdFZbx4"
      },
      "source": [
        "This cell below, takes about 10 minutes to run. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "r3lj22eBZbx4"
      },
      "outputs": [],
      "source": [
        "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1\n",
        "\n",
        "lr, num_epochs, device = 0.005, 150, DEVICE\n",
        "\n",
        "dataset = MTDataset(source, target)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=64)\n",
        "\n",
        "encoder = Encoder(len(source_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
        "decoder = Decoder(len(target_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
        "net = EncoderDecoder(encoder, decoder)\n",
        "train_seq2seq(net, dataloader, lr, num_epochs, target_vocab, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "IgdhqjpcZbx4"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "How do we know that we have obtained a good translation? BLEU (Bilingual Evaluation Understudy) is a metric that was developed specifically for this purpose. If you're curious, you can check out the details of the metric [here](https://d2l.ai/chapter_recurrent-modern/seq2seq.html?highlight=bleu#evaluation-of-predicted-sequences). For now, all you need to know is that a BLEU score lies between 0 and 1. The closer you are to 1, the better your translation is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "I_C2OSl4Zbx5"
      },
      "outputs": [],
      "source": [
        "# @markdown Compute BLEU\n",
        "def bleu(pred_seq, label_seq, k):\n",
        "  \"\"\"Compute the BLEU.\"\"\"\n",
        "  pred_tokens, label_tokens = [c for c in pred_seq], [c for c in label_seq]\n",
        "  len_pred, len_label = len(pred_tokens), len(label_tokens)\n",
        "  score = math.exp(min(0, 1 - len_label / len_pred))\n",
        "  for n in range(1, k + 1):\n",
        "    num_matches, label_subs = 0, collections.defaultdict(int)\n",
        "    for i in range(len_label - n + 1):\n",
        "      label_subs[''.join(label_tokens[i: i + n])] += 1\n",
        "    for i in range(len_pred - n + 1):\n",
        "      if label_subs[''.join(pred_tokens[i: i + n])] > 0:\n",
        "        num_matches += 1\n",
        "        label_subs[''.join(pred_tokens[i: i + n])] -= 1\n",
        "    score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))\n",
        "  return score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "EBRUwHroZbx5"
      },
      "source": [
        "Implementing a function to make a translation of a given word. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "r2whizQBZbx5"
      },
      "outputs": [],
      "source": [
        "def predict_seq2seq(net, src_sentence):\n",
        "  \"\"\"Predict for sequence to sequence.\"\"\"\n",
        "  # Set `net` to eval mode for inference\n",
        "  net.eval()\n",
        "  src_tokens = [source_vocab[c] for c in src_sentence.lower()] + [source_vocab['<eos>']]\n",
        "  # enc_valid_len = torch.tensor([len(src_tokens)], device=device)\n",
        "  src_tokens = truncate_pad(src_tokens, NUM_STEPS, source_vocab['<pad>'])\n",
        "  # Add the batch axis\n",
        "  enc_X = torch.unsqueeze(torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)\n",
        "  enc_outputs = net.encoder(enc_X)\n",
        "  dec_state = net.decoder.init_state(enc_outputs)\n",
        "  # Add the batch axis\n",
        "  dec_X = torch.unsqueeze(torch.tensor([target_vocab['<bos>']], dtype=torch.long, device=device), dim=0)\n",
        "  output_seq, attention_weight_seq = [], []\n",
        "  for _ in range(NUM_STEPS):\n",
        "    Y, dec_state = net.decoder(dec_X, dec_state)\n",
        "    # We use the token with the highest prediction likelihood as the input\n",
        "    # of the decoder at the next time step\n",
        "    dec_X = Y.argmax(dim=2)\n",
        "    pred = dec_X.squeeze(dim=0).type(torch.int32).item()\n",
        "    # Once the end-of-sequence token is predicted, the generation of the\n",
        "    # output sequence is complete\n",
        "    if pred == target_vocab['<eos>']:\n",
        "      break\n",
        "    output_seq.append(pred)\n",
        "  return output_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "YBwjTWssZbx5"
      },
      "outputs": [],
      "source": [
        "engs = ['middle', 'funny', 'hour', 'igloo', 'vendor', 'moody']\n",
        "pig_latin = ['iddlemay', 'unnyfay', 'ourhay', 'iglooway', 'endorvay', 'oodymay']\n",
        "\n",
        "column_width = 18\n",
        "\n",
        "print('English'.ljust(column_width) + 'Pig Latin'.ljust(column_width) + 'Translation'.ljust(column_width) + 'BLEU\\n--------------------------------------------------------------')\n",
        "\n",
        "for eng, pig in zip(engs, pig_latin):\n",
        "  translation = predict_seq2seq(net, eng)\n",
        "  translation = ''.join([target_idx2token[i] for i in translation])\n",
        "  print(eng.ljust(column_width) + pig.ljust(column_width) + translation.ljust(column_width) + '%.3f' % bleu(translation, pig, k=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "hOEfZLW6Zbx5"
      },
      "source": [
        "---\n",
        "# Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "R_y6iuTrZbx5"
      },
      "outputs": [],
      "source": [
        "# @title Video 8: Summary\n",
        "from ipywidgets import widgets\n",
        "\n",
        "out2 = widgets.Output()\n",
        "with out2:\n",
        "  from IPython.display import IFrame\n",
        "  class BiliVideo(IFrame):\n",
        "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
        "      self.id=id\n",
        "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
        "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "  video = BiliVideo(id=\"\", width=854, height=480, fs=1)\n",
        "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
        "  display(video)\n",
        "\n",
        "out1 = widgets.Output()\n",
        "with out1:\n",
        "  from IPython.display import YouTubeVideo\n",
        "  video = YouTubeVideo(id=\"J4zok8b2SUA\", width=854, height=480, fs=1, rel=0)\n",
        "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "  display(video)\n",
        "\n",
        "out = widgets.Tab([out1, out2])\n",
        "out.set_title(0, 'Youtube')\n",
        "out.set_title(1, 'Bilibili')\n",
        "\n",
        "# Add event to airtable\n",
        "atform.add_event('Video 8: Summary')\n",
        "\n",
        "display(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "FNVvczYRZbx5"
      },
      "source": [
        "In this tutorial, we went through applications of Recurrent Neural Networks for Natural Language Processing.\n",
        "\n",
        "We introduced a basic RNN architecture and how it compares with CNNs to demonstrate the advantage of recurrence. The downside of RNNs is that they're prone to the issue of vanishing gradients while processing long-range dependencies. More complex architectures of RNNs are used in practice to tackle this problem, such as Gated Recurrent Units(GRU) [[PyTorch - nn.GRU()]](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html) and Long Short-term Memory (LSTM) [[PyTorch - nn.LSTM()]](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html).\n",
        "\n",
        "Finally, we explored three applications:\n",
        "* Text Generation\n",
        "* Sequence Tagging\n",
        "* Language Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "nNkvlSraZbx6"
      },
      "outputs": [],
      "source": [
        "# @title Airtable Submission Link\n",
        "from IPython import display as IPydisplay\n",
        "IPydisplay.HTML(\n",
        "   f\"\"\"\n",
        " <div>\n",
        "   <a href= \"{atform.url()}\" target=\"_blank\">\n",
        "   <img src=\"https://github.com/NeuromatchAcademy/course-content-dl/blob/main/tutorials/static/SurveyButton.png?raw=1\"\n",
        " alt=\"button link end of day Survey\" style=\"width:410px\"></a>\n",
        "   </div>\"\"\" )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "G1M0ywOIZbx6"
      },
      "source": [
        "---\n",
        "# Bonus 1: Exploring input length for RNNs\n",
        "\n",
        "Let's further explore how do different models perform based on length of the text we use as input. We'll use the IMDB dataset and the Vanilla RNN model defined in Section 1.\n",
        "\n",
        "Let's increase the `sentence_length` to see how RNN performs when long reviews are allowed. We have set the length to 200 words, feel free to experiment with the length!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "llljfaXxZbx6"
      },
      "outputs": [],
      "source": [
        "TEXT, vocab_size, train_iter, valid_iter, test_iter = load_dataset(50)\n",
        "TEXT_long, vocab_size_long, train_iter_long, valid_iter_long, test_iter_long = load_dataset(200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "Y4L1S33NZbx6"
      },
      "outputs": [],
      "source": [
        "# Model hyperparamters\n",
        "learning_rate = 0.0002\n",
        "output_size = 2\n",
        "hidden_size = 100\n",
        "embedding_length = 100\n",
        "epochs = 10\n",
        "\n",
        "# Train the Vanilla RNN on the datasets\n",
        "print('Training Vanilla RNN on sentence_length = 50\\n')\n",
        "# Initialize model, training and testing\n",
        "vanilla_rnn_model = VanillaRNN(output_size, hidden_size, vocab_size, embedding_length)\n",
        "vanilla_rnn_model.to(DEVICE)\n",
        "vanilla_rnn_start_time = time.time()\n",
        "vanilla_train_loss, vanilla_train_acc, vanilla_validation_loss, vanilla_validation_acc = train(vanilla_rnn_model, DEVICE, train_iter, valid_iter, epochs, learning_rate)\n",
        "print(f\"--- Time taken to train = {time.time() - vanilla_rnn_start_time} seconds ---\")\n",
        "test_accuracy = test(vanilla_rnn_model, DEVICE, test_iter)\n",
        "print(f'Test Accuracy: {test_accuracy}%')\n",
        "# Number of model parameters\n",
        "paramters = count_parameters(vanilla_rnn_model)\n",
        "print(f'Number of parameters = {paramters}')\n",
        "\n",
        "print('Training Vanilla RNN on sentence_length = 200\\n')\n",
        "# Initialize model, training, testing\n",
        "vanilla_rnn_model_long = VanillaRNN(output_size, hidden_size, vocab_size_long, embedding_length)\n",
        "vanilla_rnn_model_long.to(DEVICE)\n",
        "vanilla_rnn_start_time_long = time.time()\n",
        "vanilla_train_loss_long, vanilla_train_acc_long, vanilla_validation_loss_long, vanilla_validation_acc_long = train(vanilla_rnn_model_long, DEVICE, train_iter_long, valid_iter_long, epochs, learning_rate)\n",
        "print(f\"--- Time taken to train = {time.time() - vanilla_rnn_start_time} seconds ---\")\n",
        "test_accuracy = test(vanilla_rnn_model_long, DEVICE, test_iter_long)\n",
        "print(f'Test Accuracy: {test_accuracy}%')\n",
        "# Number of parameters\n",
        "paramters = count_parameters(vanilla_rnn_model_long)\n",
        "print(f'Number of parameters = {paramters}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "FGF1hZ1eZbx6"
      },
      "outputs": [],
      "source": [
        "# Compare accuracies of model trained on different sentence lengths\n",
        "\n",
        "x_ticks = np.arange(epochs)\n",
        "plt.plot(x_ticks, vanilla_train_acc, label='train accuracy,len=50')\n",
        "plt.plot(x_ticks, vanilla_train_acc_long, label='train accuracy,len=200')\n",
        "\n",
        "plt.plot(x_ticks, vanilla_validation_acc, label='validation accuracy,len=50')\n",
        "plt.plot(x_ticks, vanilla_validation_acc_long, label='validation accuracy,len=200')\n",
        "\n",
        "plt.legend(loc='upper left')\n",
        "plt.xlabel('epoch')\n",
        "plt.title(\"Training and Validation Accuracy for Sentence Lengths 50 and 200\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "R1LsrBS8Zbx6"
      },
      "source": [
        "---\n",
        "# Bonus 2: Improving Text Generation\n",
        "\n",
        "As seen in Section 2.1, choosing the character with the highest probability at each time step did not fully allow us to explore language variability. For this, we must also let the model \"explore\" other character choices. One of the ways to do this is to sample from a probability distribution. \n",
        "\n",
        "Implement the function to generate text again, but we cast the output to a probability distribution this time. Your task is to sample a character from this distribution. Use the PyTorch [multinomial function](https://pytorch.org/docs/stable/generated/torch.multinomial.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "_tU-FLKcZbyA"
      },
      "source": [
        "**Network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "D1FsAkh-ZbyB"
      },
      "outputs": [],
      "source": [
        "class GenerationRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "    super(GenerationRNN, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "    self.encoder = nn.Embedding(input_size, hidden_size)\n",
        "    self.rnn = nn.RNN(hidden_size, hidden_size, n_layers)\n",
        "    self.decoder = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, input, hidden):\n",
        "    input = self.encoder(input.view(1, -1))\n",
        "    output, hidden = self.rnn(input, hidden)\n",
        "    output = self.decoder(hidden.view(1, -1))\n",
        "    return output, hidden\n",
        "\n",
        "  def init_hidden(self):\n",
        "    return torch.zeros(self.n_layers, 1, self.hidden_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "N7Vtt-IGZbyB"
      },
      "source": [
        "### Bonus Coding Exercise 1: Improving Text Generation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "q0WT12ZJZbyB"
      },
      "outputs": [],
      "source": [
        "def evaluateMultinomial(net, prime_str, predict_len, temperature=0.8):\n",
        "  hidden = net.init_hidden()\n",
        "  predicted = prime_str\n",
        "\n",
        "  # \"Building up\" the hidden state\n",
        "  for p in range(len(prime_str) - 1):\n",
        "    inp = char_tensor(prime_str[p])\n",
        "    _, hidden = net(inp, hidden)\n",
        "\n",
        "  # Tensorize of the last character\n",
        "  inp = char_tensor(prime_str[-1])\n",
        "\n",
        "  # For every index to predict\n",
        "  for p in range(predict_len):\n",
        "    ####################################################################\n",
        "    # Fill in missing code below (...),\n",
        "    # then remove or comment the line below to test your function\n",
        "    raise NotImplementedError(\"Generation Improve\")\n",
        "    ####################################################################\n",
        "    # Pass the character + previous hidden state to the model\n",
        "    output, hidden = ...\n",
        "\n",
        "    # Sample from the network as a multinomial distribution\n",
        "    output_dist = output.data.view(-1).div(temperature).exp()\n",
        "    top_i = ...\n",
        "\n",
        "    # Add predicted character to string and use as next input\n",
        "    predicted_char = all_characters[top_i]\n",
        "    predicted += predicted_char\n",
        "    inp = char_tensor(predicted_char)\n",
        "\n",
        "  return predicted\n",
        "\n",
        "\n",
        "## Uncomment to run\n",
        "# sampleDecoder = GenerationRNN(27, 100, 27, 1)\n",
        "# text = evaluateMultinomial(sampleDecoder, 'hi', 10)\n",
        "# if text.startswith('hi') and len(text) == 12:\n",
        "#   print('Success!')\n",
        "# else:\n",
        "#   print('Need to change.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "4IkBZepNZbyB"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W2D5_TimeSeriesAndNaturalLanguageProcessing/solutions/W2D5_Tutorial2_Solution_2af79004.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "tCc_mCReZbyB"
      },
      "outputs": [],
      "source": [
        "# @title Execute cell to train the model\n",
        "\n",
        "def grad_clipping(net, theta):\n",
        "  \"\"\"Clip the gradient.\"\"\"\n",
        "  params = [p for p in net.parameters() if p.requires_grad]\n",
        "\n",
        "  norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
        "\n",
        "  if norm > theta:\n",
        "    for param in params:\n",
        "      param.grad[:] *= theta / norm\n",
        "\n",
        "\n",
        "# Single training step\n",
        "def train_simple2(inp, target, chunk_len):\n",
        "  # Initialize hidden state, zero the gradients of decoder\n",
        "  hidden = decoder.init_hidden()\n",
        "  decoder.zero_grad()\n",
        "  loss = 0\n",
        "\n",
        "  # For each character in our chunk (except last), compute the hidden and ouput\n",
        "  # Using each output, compute the loss with the corresponding target\n",
        "  for c in range(chunk_len):\n",
        "    output, hidden = decoder(inp[c], hidden)\n",
        "    loss += criterion(output, target[c].unsqueeze(0))\n",
        "\n",
        "  # Backpropagate, clip gradient and optimize\n",
        "  loss.backward()\n",
        "  grad_clipping(decoder, 1)\n",
        "  decoder_optimizer.step()\n",
        "\n",
        "  # Return average loss\n",
        "  return loss.data.item() / chunk_len\n",
        "\n",
        "\n",
        "n_epochs = 3000\n",
        "hidden_size = 100\n",
        "n_layers = 1\n",
        "lr = 0.0005\n",
        "print_every, plot_every = 500, 10\n",
        "\n",
        "# Create model, optimizer and loss function\n",
        "decoder = GenerationRNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "all_losses = []\n",
        "loss_avg = 0\n",
        "\n",
        "# For every epoch\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  # Get a random (input, target) pair from training set and perform one training iteration\n",
        "  loss = train_simple2(*random_training_set(chunk_len), chunk_len)\n",
        "  loss_avg += loss\n",
        "\n",
        "  if epoch % print_every == 0:\n",
        "    text = evaluateMultinomial(decoder, 'th', 50)\n",
        "    print(f'Epoch {epoch} --------------------\\n\\t {text}')\n",
        "\n",
        "  if epoch % plot_every == 0:\n",
        "    all_losses.append(loss_avg / plot_every)\n",
        "    loss_avg = 0\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training loss for text generation')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "W2D5_Tutorial2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2010176e30244c4693d64ee394ee4af2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TabModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TabModel",
            "_titles": {
              "0": "Youtube",
              "1": "Bilibili"
            },
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TabView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4b8f12585b734ae4a4e6059d37838bfd",
              "IPY_MODEL_99d42bf5fc7246ecafba98deb9be6a1c"
            ],
            "layout": "IPY_MODEL_051e063246564f84a36abc76e79ddf50",
            "selected_index": 0
          }
        },
        "4b8f12585b734ae4a4e6059d37838bfd": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_4dcb9cdf99ff40918eeea943d7d13d35",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "Video available at https://youtube.com/watch?v=LSMPdQvkXuk\n"
                ]
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.lib.display.YouTubeVideo at 0x7f4c803efd90>",
                  "text/html": "\n        <iframe\n            width=\"854\"\n            height=\"480\"\n            src=\"https://www.youtube.com/embed/LSMPdQvkXuk?fs=1&rel=0\"\n            frameborder=\"0\"\n            allowfullscreen\n        ></iframe>\n        ",
                  "image/jpeg": "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\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "99d42bf5fc7246ecafba98deb9be6a1c": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_fd2aa03ad65c461180227591b2db1bd5",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "Video available at https://www.bilibili.com/video/\n"
                ]
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<__main__.BiliVideo at 0x7f4c8037c650>",
                  "text/html": "\n        <iframe\n            width=\"854\"\n            height=\"480\"\n            src=\"https://player.bilibili.com/player.html?bvid=&page=1?fs=1\"\n            frameborder=\"0\"\n            allowfullscreen\n        ></iframe>\n        "
                },
                "metadata": {}
              }
            ]
          }
        },
        "051e063246564f84a36abc76e79ddf50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4dcb9cdf99ff40918eeea943d7d13d35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd2aa03ad65c461180227591b2db1bd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e5d884286044fa5a4033987431eaa5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TabModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TabModel",
            "_titles": {
              "0": "Youtube",
              "1": "Bilibili"
            },
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TabView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8fd4468d834941229cf65021bfb6bcea",
              "IPY_MODEL_f2176242fa2149d6a3d2c0f49687f2a9"
            ],
            "layout": "IPY_MODEL_5e3b2a12779742539ac194b38f9452d4",
            "selected_index": 0
          }
        },
        "8fd4468d834941229cf65021bfb6bcea": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_22d7a80c038f4751bd1c3cd65265ba40",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "Video available at https://youtube.com/watch?v=T2jzzdSVJI0\n"
                ]
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.lib.display.YouTubeVideo at 0x7f4c7a2347d0>",
                  "text/html": "\n        <iframe\n            width=\"854\"\n            height=\"480\"\n            src=\"https://www.youtube.com/embed/T2jzzdSVJI0?fs=1&rel=0\"\n            frameborder=\"0\"\n            allowfullscreen\n        ></iframe>\n        ",
                  "image/jpeg": "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\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "f2176242fa2149d6a3d2c0f49687f2a9": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_186e0442bbd943fa981e47c852054715",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "Video available at https://www.bilibili.com/video/\n"
                ]
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<__main__.BiliVideo at 0x7f4c8037c1d0>",
                  "text/html": "\n        <iframe\n            width=\"854\"\n            height=\"480\"\n            src=\"https://player.bilibili.com/player.html?bvid=&page=1?fs=1\"\n            frameborder=\"0\"\n            allowfullscreen\n        ></iframe>\n        "
                },
                "metadata": {}
              }
            ]
          }
        },
        "5e3b2a12779742539ac194b38f9452d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22d7a80c038f4751bd1c3cd65265ba40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "186e0442bbd943fa981e47c852054715": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9980375a92e41e2a5c01469f52b33c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TabModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TabModel",
            "_titles": {
              "0": "Youtube",
              "1": "Bilibili"
            },
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TabView",
            "box_style": "",
            "children": [
              "IPY_MODEL_384a15d32c904375a3c2880001aca63c",
              "IPY_MODEL_1cf6392c1c1d48a090660779cb9ad5de"
            ],
            "layout": "IPY_MODEL_b95df6511f51488d8cc98fe3b98ea733",
            "selected_index": 0
          }
        },
        "384a15d32c904375a3c2880001aca63c": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_524ea3f3c93b4febab52a56af1a7eeb3",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "Video available at https://youtube.com/watch?v=JAjuG0o4_Xc\n"
                ]
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.lib.display.YouTubeVideo at 0x7f4c3ad54a10>",
                  "text/html": "\n        <iframe\n            width=\"854\"\n            height=\"480\"\n            src=\"https://www.youtube.com/embed/JAjuG0o4_Xc?fs=1&rel=0\"\n            frameborder=\"0\"\n            allowfullscreen\n        ></iframe>\n        ",
                  "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEABALDBoXFhwaGBkdHRgfIh4lIB0iIzEqJSgwLjM9MC0zMy42P1BCNjhLRS8vRWFFS1NWW11dM0JldWVYbFBZW1cBERISGBYZLxobL1c2NUJXV1hXY1djYFdZV1dXXVdXV1ddWFdXV1dXV1dkXV1XV1dXV1dXV1ddV1dXZFdjV1ddV//AABEIAWgB4AMBIgACEQEDEQH/xAAbAAEAAgMBAQAAAAAAAAAAAAAAAwcCBAYFAf/EAEUQAQACAAMCCQoEAwYFBQAAAAABAgMEERMhBRIXMVFSU5LSFjIzQWFxcpGx0QYUIlRCc4EjNDWhssEHFSR08UNigqLw/8QAGQEBAQEBAQEAAAAAAAAAAAAAAAIBBAMF/8QAHxEBAQACAgIDAQAAAAAAAAAAAAECEQMxEiEEFEET/9oADAMBAAIRAxEAPwCvwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAB1WF/wAP87atbRbA0tETH659f/xZcnme62B358IOTHWcnme62B358JyeZ7rYHfnwg5MdZyeZ7rYHfnwnJ5nutgd+fCDkx1nJ5nutgd+fCcnme62B358IOTHWcnme62B358JyeZ7rYHfnwg5MdZyeZ7rYHfnwnJ5nutgd+fCDkx1nJ5nutgd+fCcnme62B358IOTHWcnme62B358JyeZ7rYHfnwg5MdZyeZ7rYHfnwnJ5nutgd+fCDkx1nJ5nutgd+fCcnme62B358IOTHWcnme62B358JyeZ7rYHfnwg5MdZyeZ7rYHfnwnJ5nutgd+fCDkx1nJ5nutgd+fCcnme62B358IOTHWcnme62B358JyeZ7rYHfnwg5MdZyeZ7rYHfnwnJ5nutgd+fCDkxPOUtE6btx+Vt7AQCf8AK29h+Vt7AQCf8rb2H5W3sBAJ/wArb2H5W3sBAJ/ytvYflbewEAn/ACtvYflbewEAn/K29h+Vt7AQCf8AK29h+Vt7AQCf8rb2EZW3sBAPU/5Djdanzn7EcA43Ww/nP2B5Y9ePw7jdbD+c/ZnX8MY8/wAWH85+wPFHv1/COYnmvg/O3hSx+Cc1P8eD3reEHNjp/IXNdpgd63hYz+CM1H/qYHet4Qc0Oi8jMz18HvW8J5G5nr4Pet4Qc6Oi8jMz18HvW8LG/wCEMxXnvg963hBbGS9DhfBT6J0GS9DhfBT6JwAAAAAAAAAAAAAAAAAAAAAAAAAAAAUriedPvlgyxPOn3yxEgAAAAAAAAAAAAAD7Xnh8ZV5498A6am9JxGcU3pOKmqY4VNE9IfKRuS0gany9Xo4cNPAhu0bGaZ6Ir1MXOUpMxM749UIrZ7CmJmbxWI55tMRHzNr8Mu9PlqsJhjXPYN7xSt4taddNNfVv5+ZLaGM6RoMzzNiYQ5mP0n4OiyXocL4KfROgyXocL4KfROpIAAAAAAAAAAAAAAAAAAAAAAAAAAAClcTzp98sGeJ50++WAkAAAAAAAAAAAAAAZV5498MWVOePfAOz03stD1stEVcfaQlpDCkJaA2cGGWevNcG81mYtEbphjho+FMS1cC01jXm43sjp+ivxs7c5XO2jE0tpMTPN6/f0pr5idrNYvETH8O71f5tTMYuHFuNpv1jjTENnCx8O9pvFdd/PoiOzKXT28C3Gmk//uZvTDn8pi2jFwtY3WtPFnXm36OimHrpxXtHMIMxH6ZbFkOP5sss9Mj3cl6HC+Cn0ToMl6HC+Cn0TgAAAAAAAAAAAAAAAAAAAAAAAAAAAApXE86ffLBniedPvlgJAAAAAAAAAAAAAAGVOePfDFlTnj3wDtY52aOOdJCLHpGdUlXl5/hvAy1uJicabaROlY1efifjDDjzMG8++Yj7tkY6m2JxazM+qJnT3Odp+Kq5i84OxniX0rWZnfv6f6vOxvxhe0TFcGsaxMb7TP2aHBfB2LN6Xn+ziJrNZtGszpO7d0K0y5SdtvN4k8aaTM1nmtu9fQmy+tNP1TPTExpuY8J3tGNN91rbtd3Pu05kmBjWvprzIx06Mrl1XS5nMflcnOJWlbYlKxPv39Me94XAfD+NjZmYvadLazprMxHuj1Ps494iY1ma6RunfD7SKceMTixx43axune6f518v7k37jqoxNWOLO6WvgYuukxKS886NOqWX3HQ5L0OF8FPonQZL0OF8FPonQoAAAAAAAAAAAAAAAAAAAAAAAAAAABSuJ50++WDPE86ffLASAAAAAAAAAAAAAAMq88e+GL7Xnj3wDsotvT1l5f5/C19JT5s8ThbBpHnxb2V3mlOa/E99c5eOrFI/wDrE/7vJb3CmJONmMTEis6WmNPlo18DAmb1i0TFdY1mej1tHQ8F8H0wqRa0RbFnSdZ/h9ke32ty06tSM1WONPGjX1b2dc1T13rr73RPGR8fknJll5UzGHFp6J9bHDw+LM6cz7bMUmfOr83z8xh9avzTOPCXbqvyua4eOk1p/wBmM20RTj069efpQ5nHrNLcW0TOk+t63KOTHjyt1p7GRzemker1vYpiRMaxOricHN6adEaRH+8vXyPCFI1m2JEezWHjnlNbdnx8OTHLwnSxcl6HC+Cn0ToMl6HC+Cn0TvF2gAAAAAAAAAAAAAAAAAAAAAAAAAAAKVxPOn3ywZ4nnT75YCQAAAAAAAAAAAAAAAABoPr4A+vgMAAAAH18AFzZL0OF8FPonQZL0OF8FPonFAAAAAAAAAAAAAAAAAAAAAAAAAAAAKVxPOn3ywZ4nnT75YCQAAAAAAAAAAAAAAAAAAAAAAAAAAAFzZL0OF8FPonQZL0OF8FPonFAAAAA8XNfijLYOJel9rxqTMWmMK0xu59+jDB/FuUvNYrtp40xETsrab93PoD3Rq4fCGDbHvl63icalYtam/WInm+sfOG0AAAAAMMXErSs2vaK1rEza0zpERHPMy8O34wykazG2nDjXXFjCtxPnoD3xBlM3h4+HGJg3rek81ond/5TgAxxLxWs2tOlaxMzM+qI5wZCDJZzDzGFXFwbcbDtrxbaTHNOk7p388SnAAAAAAAABSuJ50++WDPE86ffLASAAAAAAAAAAAAAAAAAAAAAAAAAAAAubJehwvgp9E6DJehwvgp9E4oAAABp8Mf3TMfycX/TLW/C3+HZb+XVs8Mf3TMfycX/AEy1fwt/h2W/l1Ayubm3CWYwuLSIphYNotFf1zrrumfXDQyGfz+bjGjCnAwq4eNi4cYlqzaZ4s6REV1+c+3mT5H/ABjN/wAnL/7vn4Q9Fmv+8zP1gEPBnCGfzmFaKbDCvhXvh4mJMTaLWrPNWuu6OmZn3Q2uBeGbXyuPiZri1vlsTFpizXzZ4m+ZiEX4Q9Hm/wDvMx9YaXBmUnMZbhXCpMca+azVa9GukaA3cri8I5rDjHpbAwKXjjYWFek3tNZ5uPbWNJn2Q3+BuEpzOHbj02eNhXth4tNdYi0dE+uJ54c3wZicHzgxXM4+LgZjDiK4uFiZjEpMWiN+kcbm6NHv/h/Ay0YVsXKxicTFtrNsSbzNtN2v69+ntB9/E2RxMzkcbBwp/tLRWY36a6TE6f10ebk/xZgYVaYOawcTK3iIrxbUnibt27T1f0e3wpnZy+DbFrg3xuLMa0p52mu+f6Q8zE/FPBuJgzOJjUmkx+rDvWeN7uLMbwbt83lsrk7Y+HFPy9YteNnppbWfVpu1mZaWF/zTEw4xotlsOZjjVy9qWnd6otfXn90PHyHBONjcD5rDpS1YxMW2Jl8O3PxYmLVjf08WfqmymPwZfCi2JmcbCvEfrwsTM4tb1mOeOLNtZ/oD08Xh61uDMXNYdYpi4cWi2HbfFb1nS0Tpzpsjj5zErbGx64VMGcKZphRra+ukTE2nm6d0dPO83hDL4GHwLmbZat64eLW2JO043GmZmImZ42/fpD36/wB1j+VH+kHk5LhuMLgnCzV8OvGmsRXCw44sTabTFa1j1M7RwpWm142WtaI1nLRS0burGJrz/wBNNXj1yN8fgDLbOs3vhTXE4kTMTaK2tExExv10mebfuT0zHBU4e0/N4sbt9JzOLtIno4nG11Bv578R/wDR5fHy9a8bMXph0nE82k211m+nqjSW1l8PP0xKcfFwMbCtP69KTh2rHTG+Yn3NfFjI5bI4dMTCtGUxJjdetrcXjfq1vrvrv9fql5H/AE+XzOWjgvM2vN8WtcTL0xNph7P+K0xOvF094O1AAAAABSuJ50++WDPE86ffLASAAAAAAAAAAAAAAAAAAAAAAAAAAAAubJehwvgp9E6DJehwvgp9E4oAAAB8mCI03RzPoD5pGuum/pIrEc0aPoD5FYjmjQiIjmh9AR4mBS062pW0xzTMRMs30ARTlsObcacOnG6eLGvzSgCK2Xpa3GmlZt0zWNfmlAfJjXdO+DR9AfIiI3RuhHsKcbjcSvG63FjX5pQHyY13TzMMPApTXiUrXXn0iISAAAAAAAKVxPOn3ywZ4nnT75YCQAAAAAAAAAAAAAAAAAAAAAAAAAAAFzZL0OF8FPonQZL0OF8FPonFAAAAAAAAAAAAAAAAAAAAAAAAAAAAKVxPOn3ywZ4nnT75YCQAAAAAAAAAAAAAAAAAAAAAAAAAAAFzZL0OF8FPonQZL0OF8FPonFAAAAAAAAAAAAAAAAAAAAAAAAAAAAKVxPOn3ywZ4nnT75YCQAAAAAAAAAAAAAAAAAAAAAAAAAAAFzZL0OF8FPonQZL0OF8FPonFAAAAAAAAAAAAAAAAAAAAAAAAAAAAKVxPOn3ywZ4nnT75YCQAAAAAAAAAAAAAAAAAAAAAAAAAAAFzZL0OF8FPonQZL0OF8FPonFAAAAAAAAAAAAAAAAAAAAAAAAAAAAKVxPOn3ywZ4nnT75YCQAAAAAAAAAAAAAAAAAAAAAAAAAAAFzZL0OF8FPoneVlMe2yw9/8ABT6JtvfrCm+NDb36xt79YG+NDb36xt79YG+NDb36xt79YG+NDb36xt79YG+NDb36xt79YG+NDb36xt79YG+NDb36xt79YG+NDb36xt79YG+NDb36xt79YG+NDb36xt79YG+NDb36xt79YG+NDb36xt79YG+NDb36xt79YG+NDb36yHMZ+2HEfq3zOkazER0azPqjfHzgFVYnnT75YO04S4MwcPDtauVjEmY5sOttf1zpW0WmdJmN+72xzev0MLgDKTGs5akdGlpnd6pEq7Fj+T2T/b0+c/c8nsn+3p85+41XAsfyeyf7enzn7nk9k/29PnP3BXAsfyeyf7enzn7nk9k/29PnP3BXAsfyeyf7enzn7nk9k/29PnP3BXAsfyeyf7enzn7nk9k/29PnP3BXAsfyeyf7enzn7nk9k/29PnP3BXAsfyeyf7enzn7nk9k/29PnP3BXAsfyeyf7enzn7nk9k/29PnP3BXAsfyeyf7enzn7nk9k/29PnP3BXAsfyeyf7enzn7nk9k/29PnP3BXAsfyeyf7enzn7nk9k/29PnP3BXAsfyeyf7enzn7nk9k/29PnP3BXAsfyeyf7enzn7vk8AZKOfApH9Z+4K5FjV4ByU66YGHOnPpM7v8yfw/k9P7vT5z9wb2V9Fh/BT6NDO4+Lg2xrxW804uFas7pr+nWbV0mdYm26u6OeYevk8KNjh7v4KfSE2zjoGudtGdjfXzrcWZ5piJ4sbtJ/hidYnTSZ05/Wl4Rw8ecTEjDrea4uHh1ratoiKWi1uNM6zExutG+NeZ7uzjoNlXoBzuYx81hxNr20pNt+6kTWOPaI4uu6d3E5/b62OBj569azxJrrh4e+Yr50xSZni88c+JGk9DpNnHQbKvQDQ2F9rW+1txIpNZw9I0mdfO16XlYuTx8KMS2BF5vtMSacbEm0cXYzxN1raefMOk2Veg2VegHP4352szxJ49f1b5ikTGl66THt4s354/hj+vzhPCzF8HCmsXnFiJm1aTxYmdN3GmL1mPfEzp0TudDsq9Bsq9APCpOcm9otpWu0iI0isxFeNzxr/7efXXf8kdfzsbojSIpOk6Vtv0nn1mJ42unr00/wAuh2Veg2VegHhZrEzcZas4dJ2/6tYmaTzRPF13RGkzpzac75a2cjWd8xNr7qxTWtYxNK8XXnmab9/R0ve2Veg2VegHP2y+ZnDy01m1cWkWteLW3TOm6t+fWJ5t2unP6kGBg5qMvfCtXF2lsPCiLzeJiLRWItGvG11115vm6fZV6DZV6AebkMtbCpNb3488a0xz6RE80RxpmdPfLabGyr0Gyr0A1xsbKvQbKvQDXGxsq9Bsq9ANcbGyr0Gyr0A1xsbKvQbKvQDXGxsq9Bsq9ANd5fCmcrh3pHFm3GtGHOlONFdZrbWfVppE/wCT3NlXoYXy1LTEzGumum+dN8aTu9YPH8y1Z0m0zaf7K1om1efixWI+Ln9Vfc38DD4mHSuuvFrWNfdGjaplqV82kRzc27m3Qy2VegGuNjZV6DZV6Aa42NlXoNlXoBrjY2Veg2VegGuNjZV6DZV6Aa42NlXoNlXoBrjY2Veg2VegGuNjZV6DZV6Aa42NlXoNlXoBrjY2Veg2VegGuNjZV6DZV6Aa42NlXoNlXoBrjY2Veg2VegGu1s5lpxIrpaI0mJ0mNY1iY3/X5vR2Veg2VegHn4GFaL3tOkaxWIiJ15tfZGnPzb09uaWzsq9D5bCrpO4GOT9Dh/BT6QmQ5P0OH8FPpCYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAB8tzS+vluaQRZP0OH8FPpCZWWH+P83Wtaxh5fSsREfpt6t3WZcoWc7LL92/iBZYrTlCznZZfu38RyhZzssv3b+IFlitOULOdll+7fxHKFnOyy/dv4gWWK05Qs52WX7t/EcoWc7LL92/iBZYrTlCznZZfu38RyhZzssv3b+IFlitOULOdll+7fxHKFnOyy/dv4gWWK05Qs52WX7t/EcoWc7LL92/iBZYrTlCznZZfu38RyhZzssv3b+IFlitOULOdll+7fxHKFnOyy/dv4gWWK05Qs52WX7t/EcoWc7LL92/iBZYrTlCznZZfu38RyhZzssv3b+IFlitOULOdll+7fxHKFnOyy/dv4gWWK05Qs52WX7t/EcoWc7LL92/iBZYrTlCznZZfu38RyhZzssv3b+IFlitOULOdll+7fxHKFnOyy/dv4gWWK05Qs52WX7t/EcoWc7LL92/iBZYrTlCznZZfu38RyhZzssv3b+IFlitOULOdll+7fxHKFnOyy/dv4gWWK05Qs52WX7t/EcoWc7LL92/iBZYrTlCznZZfu38RyhZzssv3b+IFlitOULOdll+7fxHKFnOyy/dv4gWWK05Qs52WX7t/EcoWc7LL92/iBZYrTlCznZZfu38RyhZzssv3b+IFlitOULOdll+7fxHKFnOyy/dv4gWWK05Qs52WX7t/EcoWc7LL92/iBZYrTlCznZZfu38RyhZzssv3b+IFlitOULOdll+7fxHKFnOyy/dv4gWWK05Qs52WX7t/EcoWc7LL92/iBZYrTlCznZZfu38RyhZzssv3b+IFlvluaVa8oWc7LL92/iJ/4hZzssv3b+IHJgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/9k=\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "1cf6392c1c1d48a090660779cb9ad5de": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_75a416f8353d470b895f8b9fbbe05a00",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "Video available at https://www.bilibili.com/video/\n"
                ]
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<__main__.BiliVideo at 0x7f4c3ad50fd0>",
                  "text/html": "\n        <iframe\n            width=\"854\"\n            height=\"480\"\n            src=\"https://player.bilibili.com/player.html?bvid=&page=1?fs=1\"\n            frameborder=\"0\"\n            allowfullscreen\n        ></iframe>\n        "
                },
                "metadata": {}
              }
            ]
          }
        },
        "b95df6511f51488d8cc98fe3b98ea733": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "524ea3f3c93b4febab52a56af1a7eeb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75a416f8353d470b895f8b9fbbe05a00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}